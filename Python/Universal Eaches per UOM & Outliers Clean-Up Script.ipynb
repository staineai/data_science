{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BACKGROUND\n",
    "The biggest problem faced by analysts at Lumere is the need to clean up very messy client data quickly and efficiently. This script allows an analyst to clean raw client data so that it is ready for analysis. Some queries and usernames have been scrubbed since they are not for public view. This script is to showcase my ability to solve large data problems using Python libaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Update inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary packages and set up warehouse connection: \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from __future__ import print_function\n",
    "import getpass\n",
    "import pymysql\n",
    "import os\n",
    "import fnmatch\n",
    "import psycopg2\n",
    "import math\n",
    "\n",
    "pymysql.install_as_MySQLdb()\n",
    "\n",
    "def fetch(cur):\n",
    "    df = pd.DataFrame(np.array(cur.fetchall()))\n",
    "    colnames = [desc[0] for desc in cur.description]\n",
    "    df.columns = colnames\n",
    "    return df\n",
    "\n",
    "ssl = {'UPDATE KEY'}\n",
    "\n",
    "USERNAME = 'UPDATE USERNAME'\n",
    "your_id = UPDATE ID\n",
    "path = 'UPDATE PATH'\n",
    "csv_file_name = 'UPDATE FILE NAME'\n",
    "\n",
    "_OPPORTUNITY_ID_ = UPDATE OPPORTUNITY id\n",
    "_PROVIDER_ID_ = UPDATE PROVIDER ID\n",
    "\n",
    "\n",
    "PASSWORD_WAR = getpass.getpass()\n",
    "CLEAN_UP_TYPE = input('select opportunity, dashboard, or ROI: ')\n",
    "UPDATE_ALL_EUOM = input('update all e/uom regardless of type clean up(T/F): ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Aggregate SmartQA files for dashboard or ROI level clean up & upload any new data ID's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if CLEAN_UP_TYPE == 'opportunity':\n",
    "    pass\n",
    "else:\n",
    "    #Aggregate excel files in provider folder\n",
    "    excel_smart_qa_files = [os.path.join(dirpath, f)\n",
    "                for dirpath, dirnames, files in os.walk(path+'/SmartQA Output')\n",
    "                for f in fnmatch.filter(files, '*.xlsx')]\n",
    "\n",
    "    def all_missing(_list_):\n",
    "        import os\n",
    "        import pandas as pd\n",
    "        df_list = []\n",
    "        for file in _list_:\n",
    "            if file.endswith(\".xlsx\"):\n",
    "                df_temp = pd.read_excel(file, sheetname = \"All_Missing\")\n",
    "\n",
    "                cols = [c for c in df_temp.columns.values if c.lower()[:7] != 'unnamed']\n",
    "                df_temp = df_temp[cols]\n",
    "\n",
    "                df_list.append(df_temp)\n",
    "        df_out = pd.concat(df_list)\n",
    "        return df_out\n",
    "\n",
    "    smart_qa_data = all_missing(excel_smart_qa_files)\n",
    "\n",
    "    # automatically select any line with probability greater than 95% to include: \n",
    "    correct_matches = smart_qa_data.loc[smart_qa_data['random_forest_probability_agg'] >= .95]\n",
    "    correct_matches = correct_matches.sort_values(by=['id','random_forest_probability_agg']).drop_duplicates(['id'],keep='first').reset_index(drop=True)\n",
    "    id_list = tuple(correct_matches['id'])\n",
    "    _SMARTQA_ID_LIST_ = []\n",
    "\n",
    "    for i in id_list: \n",
    "        x = int(i)\n",
    "        _SMARTQA_ID_LIST_.append(x)\n",
    "\n",
    "    _SMARTQA_ID_LIST_ = tuple(_SMARTQA_ID_LIST_)\n",
    "\n",
    "    print(len(smart_qa_data),'total rows')\n",
    "    print(len(correct_matches),'total rows with probability greater than 95%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RUN THIS EVEN IF YOU ARE DOING OPPORTUNITY LEVEL CLEAN UP#\n",
    "# connect to warehouse\n",
    "con_war = psycopg2.connect(dbname = 'UPDATE DATABASE NAME',\\\n",
    "                host = 'UPDATE HOST NAME',\\\n",
    "                user = USERNAME,\\\n",
    "                password = PASSWORD_WAR,\\\n",
    "                port = 'UPDATE PORT')\n",
    "cur_war = con_war.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if CLEAN_UP_TYPE == 'opportunity':\n",
    "    pass\n",
    "else: \n",
    "    # Pull any data ids that are already in the include_roi_smartqa table or are in the po_poterm_po_data table\n",
    "    def include_smart_qa(SMARTQA_ID_LIST): \n",
    "        cur_war.execute('''SELECT\n",
    "      pd.id,\n",
    "      CASE WHEN pd.id IN (SELECT data_id\n",
    "                          FROM include_roi_smartqa)\n",
    "        THEN 1\n",
    "      ELSE 0\n",
    "      END\n",
    "        AS in_include_roi_smartqa,\n",
    "      CASE WHEN isq.probability iS NULL\n",
    "        THEN 0\n",
    "      ELSE isq.probability\n",
    "      END\n",
    "        AS probability,\n",
    "      CASE WHEN pd.id IN (SELECT data_id\n",
    "                          po_poterm_po_data)\n",
    "        THEN 1\n",
    "      ELSE 0\n",
    "      END\n",
    "        AS in_po_poterm_po_data\n",
    "    FROM db_15582_public.po_data pd\n",
    "      LEFT JOIN include_roi_smartqa isq\n",
    "        ON pd.id = isq.data_id\n",
    "    WHERE id IN %(SMARTQA_ID_LIST)s;''',\n",
    "                {\n",
    "                    'SMARTQA_ID_LIST' : SMARTQA_ID_LIST\n",
    "                }\n",
    "                )\n",
    "        df_out = fetch(cur_war)\n",
    "        return df_out\n",
    "    \n",
    "    # Exclude any data ids that are the the po_poterm_po_data table\n",
    "    # Update any data ids that appear in the include_roi_smartqa table and have a higher probability\n",
    "    smart_qa_ids = include_smart_qa(SMARTQA_ID_LIST = _SMARTQA_ID_LIST_)\n",
    "    smart_qa_ids = pd.merge(correct_matches, smart_qa_ids, how='outer', on = 'id',left_index=False, right_index=False)\n",
    "    smart_qa_ids_to_include = smart_qa_ids.loc[(smart_qa_ids['in_po_poterm_po_data']==0)&(smart_qa_ids['in_include_roi_smartqa']==0)].reset_index(drop=True)\n",
    "    smart_qa_ids_to_update = smart_qa_ids.loc[(smart_qa_ids['in_include_roi_smartqa']==1)&(smart_qa_ids['probability']<smart_qa_ids['random_forest_probability_agg'])].reset_index(drop=True)\n",
    "\n",
    "    text_file_1 = open(csv_file_name+'_insert_statements_include_from_smartQA.txt','w')\n",
    "    insert_groups_and_ids = smart_qa_ids_to_include[['id','group_predict','random_forest_probability_agg']]\n",
    "    for value in range(len(insert_groups_and_ids)):\n",
    "        print('INSERT INTO include_roi_smartqa (data_id,productgroup_id,probability) VALUES (',insert_groups_and_ids.loc[value,'id'],',',insert_groups_and_ids.loc[value,'group_predict'],',',insert_groups_and_ids.loc[value,'random_forest_probability_agg'],');\\n', end = '', file = text_file_1)\n",
    "    text_file_1.close()\n",
    "\n",
    "    text_file_2 = open(csv_file_name+'_update_statements_include_from_smartQA.txt','w')\n",
    "    update_groups_and_ids = smart_qa_ids_to_update[['id','group_predict','random_forest_probability_agg']]\n",
    "    for value in range(len(update_groups_and_ids)):\n",
    "        print('UPDATE include_roi_smartqa SET productgroup_id = ',update_groups_and_ids.loc[value,'group_predict'],', probability = ',update_groups_and_ids.loc[value,'random_forest_probability_agg'],' WHERE data_id = ',update_groups_and_ids.loc[value,'id'],';\\n', end = '', file = text_file_2)\n",
    "    text_file_2.close()\n",
    "    \n",
    "    print(len(smart_qa_ids_to_include),'total rows added to include_roi_smartqa table')\n",
    "    print(len(smart_qa_ids_to_update),'total rows updated in include_roi_smartqa table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Pull raw benchmarking data and verify that it is ready to import into the python script\n",
    "\n",
    "1. Verify types look correct, improve type coverage if necessary.\n",
    "2. For any groups with intentionally blank types, change the type name in the platform/benchmkarking data to something meaningful. This script will change blank types to \"No type\", keep these rows to check for duplicates, then delete all rows with \"No type\"\n",
    "3. Edit type and product names for weird symbols - this can mess with the python script.\n",
    "4. Look for incorrect data being pulled in (e.g. gloves into staplers, leads into icd generator type). There are steps in the python script to check for this, but try to get the data as clean as you can before you run the script.\n",
    "5. Save the edited benchmarking csv file to the same folder where the jupyter notebook lives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test warehouse connection\n",
    "cur_war.execute('''TEST QUERY''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up query to pull data from periscope warehouse database\n",
    "if CLEAN_UP_TYPE == 'opportunity':\n",
    "    def opportunity(OPPORTUNITY_ID): \n",
    "        cur_war.execute('''OPPORTUNITY DATA QUERY''',\n",
    "                {\n",
    "                    'OPPORTUNITY_ID' : OPPORTUNITY_ID\n",
    "                }\n",
    "                )\n",
    "        df_out = fetch(cur_war)\n",
    "        return df_out\n",
    "    print('opportunity data pulled')\n",
    "elif CLEAN_UP_TYPE == 'dashboard':\n",
    "    def dashboard(OPPORTUNITY_ID): \n",
    "        cur_war.execute('''DASHBOARD QUERY''',\n",
    "                {\n",
    "                    'OPPORTUNITY_ID' : OPPORTUNITY_ID\n",
    "                }\n",
    "                )\n",
    "        df_out = fetch(cur_war)\n",
    "        return df_out\n",
    "    print('dashboard data pulled')\n",
    "else: \n",
    "    def roi(PROVIDER_ID): \n",
    "        cur_war.execute('''ROI QUERY''',\n",
    "                {\n",
    "                    'PROVIDER_ID' : PROVIDER_ID\n",
    "                }\n",
    "                )\n",
    "        df_out = fetch(cur_war)\n",
    "        return df_out\n",
    "    print('roi data pulled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save raw data to a csv\n",
    "if CLEAN_UP_TYPE == 'opportunity':\n",
    "    benchmarking_data_raw = opportunity(OPPORTUNITY_ID = _OPPORTUNITY_ID_)\n",
    "    benchmarking_data_raw.to_csv(csv_file_name+'.csv')\n",
    "elif CLEAN_UP_TYPE == 'dashboard':\n",
    "    benchmarking_data_raw = dashboard(OPPORTUNITY_ID = _OPPORTUNITY_ID_)\n",
    "    benchmarking_data_raw.to_csv(csv_file_name+'.csv')\n",
    "else: \n",
    "    benchmarking_data_raw = roi(PROVIDER_ID = __PROVIDER_ID__)\n",
    "    benchmarking_data_raw.to_csv(csv_file_name+'.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STOP: verify that raw data is ready for clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Import data & check that the data type printed for the 4 columns listed is either 'float64' or 'int64'. \n",
    "\n",
    "* If any are listed as 'object', there is a non-numeric entry that needs to be fixed. Clean up benchmarking data and run the script from Step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###################################### SECTION: IMPORT DATA ########################################################\n",
    "\n",
    "# Read in data\n",
    "benchmarking_data = pd.read_csv(csv_file_name+'.csv', encoding = \"ISO-8859-1\", dtype = {'vendor_catalog_number_stripped': str,'manufacturer_catalog_number_stripped': str,'vendor_catalog_number': str,'manufacturer_catalog_number': str,} )\n",
    "\n",
    "# Define percentile function\n",
    "def percentile(n):\n",
    "    def percentile_(x):\n",
    "        return np.percentile(x, n)\n",
    "    percentile_.__name__ = 'percentile_%s' % n\n",
    "    return percentile_\n",
    "\n",
    "print(len(benchmarking_data),'rows in dataset')\n",
    "benchmarking_data[['units','eaches_per_uom','price_per_each','extended_cost']].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#future - add check and fix data if scientific notation in vendor or manufacturer stripped\n",
    "#benchmarking_data[benchmarking_data['vendor_catalog_number_stripped']==\"3.58E+11\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Check that there are no duplicate values printed (po term associated with multiple products, groups, or types)\n",
    "\n",
    "* If there are, fix po terms in the platform. Re-run benchmarking or fix in benchmarking file. Start script from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add quotations to matched catalog number stripped so it is always read as a string\n",
    "benchmarking_data['matched_catalog_number_stripped'] = '\"'+benchmarking_data['matched_catalog_number_stripped']+'\"'\n",
    "\n",
    "# Check for duplicate ids\n",
    "benchmarking_data.loc[benchmarking_data['typename'].isnull(),'typename'] = 'No type'\n",
    "all_duplicates = benchmarking_data[benchmarking_data.duplicated('id', False)==True].sort_values('id')\n",
    "duplicate_count = all_duplicates.groupby(['matched_catalog_number_stripped', 'brand', 'product', 'typename', 'groupname']).agg({'id':{'id_duplicate_count': 'count'}})\n",
    "duplicate_count.columns = duplicate_count.columns.droplevel(0)\n",
    "\n",
    "# Prints all po lines that are duplicates\n",
    "#all_duplicates[['id', 'brand', 'product', 'typename', 'matched_catalog_number']]\n",
    "\n",
    "# Print count of duplicates by catalog number, product, type, and group\n",
    "duplicate_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Delete lines with extended cost <= 0 and separate lines with no type\n",
    "\n",
    "* Look at how many rows were deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(benchmarking_data[benchmarking_data['typename']=='No type']),'rows with no type')\n",
    "\n",
    "# Delete data with extended_cost <= 0 \n",
    "benchmarking_data = benchmarking_data[(benchmarking_data['extended_cost']>0)]\n",
    "benchmarking_data['price_per_each'] = benchmarking_data['extended_cost']/(benchmarking_data['units']*benchmarking_data['eaches_per_uom']).where(benchmarking_data['eaches_per_uom']>0)\n",
    "benchmarking_data.loc[benchmarking_data['price_per_each'].isnull(),'price_per_each'] = 0\n",
    "\n",
    "# Separate data with no types & check that groups with complete type coverage don't have po lines with no type\n",
    "benchmarking_data_no_types = benchmarking_data[(benchmarking_data['typename']=='No type')]\n",
    "\n",
    "no_type_check = benchmarking_data_no_types.groupby(['groupname']).agg({'type_coverage_complete': 'sum'})\n",
    "\n",
    "if max(no_type_check['type_coverage_complete']) > 0: \n",
    "    raise ValueError('There are PO lines with NO TYPE for groups that have complete type coverage')\n",
    "else:\n",
    "    pass\n",
    "\n",
    "benchmarking_data_sku = benchmarking_data.loc[(benchmarking_data['clean_up_level']=='sku-level')]\n",
    "benchmarking_data_type = benchmarking_data.loc[(benchmarking_data['clean_up_level']=='type-level')&(benchmarking_data['typename']!='No type')]\n",
    "\n",
    "if CLEAN_UP_TYPE == 'opportunity':\n",
    "    print('data with no type deleted')\n",
    "else:\n",
    "    benchmarking_data_sku = pd.concat([benchmarking_data_sku, benchmarking_data_no_types], axis=0)\n",
    "    print('data with no type kept')\n",
    "\n",
    "print(len(benchmarking_data_sku),'rows in sku dataset')\n",
    "print(len(benchmarking_data_type),'rows in type dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Determine correct e/uom values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################### SECTION: DETERMINE CORRECT E/UOM VALUES ###################################\n",
    "#This step looks at the median price for each e/uom value, and compares it to the overall median price for that product. \n",
    "#If the median value is within -25-75% of the median price, the value is allowed. \n",
    "#If there are multiple values allowed, the script allows only the mode of these.\n",
    "\n",
    "#Cutoff parameters\n",
    "percent_difference_median_lower = -25\n",
    "percent_difference_median_higher = 75\n",
    "\n",
    "##################################### TYPE LEVEL DATA #####################################\n",
    "\n",
    "if len(benchmarking_data_type) > 0:\n",
    "    # Index data\n",
    "    benchmarking_data_type_indexed = benchmarking_data_type[(benchmarking_data_type['typename'].notnull())&(benchmarking_data_type['product'].notnull())&(benchmarking_data['brand'].notnull())].sort_values(by=['groupname', 'typename', 'brand', 'product']).set_index(['groupname', 'typename', 'brand', 'product'])\n",
    "\n",
    "    # Create dataframe of counts and medians by type/brand/product combination; add min_e_per_uom column\n",
    "    median_price_by_product = benchmarking_data_type[benchmarking_data_type['price_per_each']>0].groupby(['groupname', 'typename', 'brand', 'product']).agg({'price_per_each':{'count_product': 'count', 'median_price_product': 'median'}})\n",
    "    median_price_by_product.columns = median_price_by_product.columns.droplevel(0)\n",
    "    median_price_by_product['min_e_per_uom'] = 1\n",
    "\n",
    "    # Create dataframe of counts and medians by type/brand/product/each per uom combination\n",
    "    median_price_by_uom_type = benchmarking_data_type.groupby(['groupname', 'typename', 'brand', 'product', 'eaches_per_uom']).agg({'price_per_each':{'count_uom': 'count', 'median_price_uom': 'median'}})\n",
    "    median_price_by_uom_type.columns = median_price_by_uom_type.columns.droplevel(0)\n",
    "    median_price_by_uom_type.reset_index(['eaches_per_uom'], inplace=True)\n",
    "\n",
    "    # Merge datasets, subset to products with e/uom >1\n",
    "    median_price_compare_type = pd.merge(median_price_by_product, median_price_by_uom_type, how='outer', left_index=True, right_index=True)\n",
    "    median_price_compare_type = median_price_compare_type[median_price_compare_type['eaches_per_uom']>1]\n",
    "\n",
    "    # Calculate percent difference of the median price per each within each type/brand/product/each per uom combination compared with overall product median price per each\n",
    "    median_price_compare_type['percent_difference_from_median'] = (((median_price_compare_type['median_price_uom']-median_price_compare_type['median_price_product'])/median_price_compare_type['median_price_product'])*100).round(1)\n",
    "    median_price_by_uom_info_type = median_price_compare_type.drop(['min_e_per_uom'],1)\n",
    "\n",
    "    median_price_by_uom_info_type['eaches_per_uom_value_allowed'] = 0\n",
    "    median_price_by_uom_info_type.loc[(median_price_by_uom_info_type['percent_difference_from_median']>percent_difference_median_lower)&(median_price_by_uom_info_type['percent_difference_from_median']<percent_difference_median_higher), 'eaches_per_uom_value_allowed'] = 1\n",
    "\n",
    "    # Export dataset\n",
    "    median_price_by_uom_info_type.to_csv(csv_file_name+' - median_price_by_uom_info_type '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "    # Create dataframe of correct/allowable eaches per uom values - keep any type/brand/product/each per uom combination that has a median price per each < 25% or >75% of the overall product median price per each\n",
    "    eaches_per_uom_allowed_type = median_price_compare_type[(median_price_compare_type['percent_difference_from_median']<percent_difference_median_higher)&(median_price_compare_type['percent_difference_from_median']>percent_difference_median_lower)]\n",
    "    eaches_per_uom_allowed_type = eaches_per_uom_allowed_type[['eaches_per_uom', 'count_uom']]\n",
    "    eaches_per_uom_allowed_type = eaches_per_uom_allowed_type.astype(int)\n",
    "    eaches_per_uom_allowed_type.rename(columns={'eaches_per_uom': 'max_e_per_uom', 'count_uom': 'count_max_e_per_uom'}, inplace=True)\n",
    "\n",
    "    # Merge datasets\n",
    "    all_eaches_per_uom_allowed_type = pd.merge(median_price_by_product, eaches_per_uom_allowed_type, how='outer', left_index=True, right_index=True)\n",
    "    all_eaches_per_uom_allowed_type['additional_uom_value_1'] = np.nan ###edit v4\n",
    "    all_eaches_per_uom_allowed_type['additional_uom_value_2'] = np.nan ###edit v4\n",
    "    all_eaches_per_uom_allowed_type['additional_uom_value_3'] = np.nan ###edit v4\n",
    "    all_eaches_per_uom_allowed_type['additional_uom_value_4'] = np.nan ###edit v4\n",
    "\n",
    "    # Delete multiple multiple e/uom values for type/brand/product combinations - keep most frequent\n",
    "    all_eaches_per_uom_allowed_type = all_eaches_per_uom_allowed_type.reset_index()\n",
    "    all_eaches_per_uom_allowed_type = all_eaches_per_uom_allowed_type.sort_values(by = ['groupname', 'typename', 'brand', 'product', 'count_max_e_per_uom'], ascending = False)\n",
    "    all_eaches_per_uom_allowed_type = all_eaches_per_uom_allowed_type.drop_duplicates(['groupname', 'typename', 'brand', 'product'], keep ='first')\n",
    "    all_eaches_per_uom_allowed_type = all_eaches_per_uom_allowed_type[['groupname', 'typename', 'brand', 'product', 'median_price_product', 'count_product', 'min_e_per_uom', 'max_e_per_uom', 'count_max_e_per_uom', 'additional_uom_value_1', 'additional_uom_value_2', 'additional_uom_value_3', 'additional_uom_value_4']] ###edit v4\n",
    "    #all_eaches_per_uom_allowed_type['median_price_product'] = round(all_eaches_per_uom_allowed_type['median_price_product'], 2) ###edit v2\n",
    "\n",
    "    # Set index\n",
    "    all_eaches_per_uom_allowed_type = all_eaches_per_uom_allowed_type.set_index(['groupname', 'typename', 'brand', 'product'])\n",
    "\n",
    "    # create list of all eaches per uom that show up for each group/type/brand/product, and the count for each\n",
    "    all_eaches_per_uom_type = benchmarking_data_type.groupby(['groupname', 'typename', 'brand', 'product', 'eaches_per_uom']).agg({'price_per_each':{'count_uom': 'count'}})\n",
    "    all_eaches_per_uom_type.columns = all_eaches_per_uom_type.columns.droplevel(0)\n",
    "    all_eaches_per_uom_type.reset_index(['eaches_per_uom'], inplace=True)\n",
    "    all_eaches_per_uom_type = all_eaches_per_uom_type[all_eaches_per_uom_type['eaches_per_uom']>0]\n",
    "    all_eaches_per_uom_type_int = all_eaches_per_uom_type.astype(int)\n",
    "    all_eaches_per_uom_type_str = all_eaches_per_uom_type_int.astype(str)\n",
    "    all_eaches_per_uom_type_str['e_per_uom:count'] = all_eaches_per_uom_type_str['eaches_per_uom'] + ':' + all_eaches_per_uom_type_str['count_uom']\n",
    "    all_eaches_per_uom_type_str = all_eaches_per_uom_type_str.reset_index()\n",
    "    all_eaches_per_uom_type_str = all_eaches_per_uom_type_str[['groupname', 'typename', 'brand', 'product', 'e_per_uom:count']]\n",
    "    all_eaches_per_uom_type_list = all_eaches_per_uom_type_str.groupby(['groupname', 'typename', 'brand', 'product'])['e_per_uom:count'].apply(list)\n",
    "    all_eaches_per_uom_type_list = all_eaches_per_uom_type_list.to_frame()\n",
    "\n",
    "    # Merge\n",
    "    eaches_per_uom_info_type = pd.merge(all_eaches_per_uom_allowed_type, all_eaches_per_uom_type_list, how='outer', left_index=True, right_index=True)\n",
    "    eaches_per_uom_info_type = eaches_per_uom_info_type.drop('count_max_e_per_uom', 1) ###edit v4\n",
    "    eaches_per_uom_info_type\n",
    "\n",
    "    # Export dataset\n",
    "    eaches_per_uom_info_type.to_csv(csv_file_name+' - eaches_per_uom_info_type '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "else:\n",
    "    pass\n",
    "\n",
    "##################################### SKU LEVEL DATA #####################################\n",
    "\n",
    "if len(benchmarking_data_sku) > 0:\n",
    "    # Index data\n",
    "    benchmarking_data_sku_indexed = benchmarking_data_sku[(benchmarking_data_sku['typename'].notnull())&(benchmarking_data_sku['product'].notnull())&(benchmarking_data_sku['brand'].notnull())].sort_values(by=['matched_catalog_number_stripped']).set_index(['matched_catalog_number_stripped'])\n",
    "\n",
    "    # Create dataframe of counts and medians by type/brand/product combination; add min_e_per_uom column\n",
    "    median_price_by_sku = benchmarking_data_sku[benchmarking_data_sku['price_per_each']>0].groupby(['matched_catalog_number_stripped']).agg({'price_per_each':{'count_sku': 'count', 'median_price_sku': 'median'}})\n",
    "    median_price_by_sku.columns = median_price_by_sku.columns.droplevel(0)\n",
    "    median_price_by_sku['min_e_per_uom'] = 1\n",
    "\n",
    "    # Create dataframe of counts and medians by type/brand/product/each per uom combination\n",
    "    median_price_by_uom_sku = benchmarking_data_sku.groupby(['matched_catalog_number_stripped', 'eaches_per_uom']).agg({'price_per_each':{'count_uom': 'count', 'median_price_uom': 'median'}})\n",
    "    median_price_by_uom_sku.columns = median_price_by_uom_sku.columns.droplevel(0)\n",
    "    median_price_by_uom_sku.reset_index(['eaches_per_uom'], inplace=True)\n",
    "\n",
    "    # Merge datasets, subset to products with e/uom >1\n",
    "    median_price_compare_sku = pd.merge(median_price_by_sku, median_price_by_uom_sku, how='outer', left_index=True, right_index=True)\n",
    "    median_price_compare_sku = median_price_compare_sku[median_price_compare_sku['eaches_per_uom']>1]\n",
    "\n",
    "    # Calculate percent difference of the median price per each within each type/brand/product/each per uom combination compared with overall product median price per each\n",
    "    median_price_compare_sku['percent_difference_from_median'] = (((median_price_compare_sku['median_price_uom']-median_price_compare_sku['median_price_sku'])/median_price_compare_sku['median_price_sku'])*100).round(1)\n",
    "    median_price_by_uom_info_sku = median_price_compare_sku.drop(['min_e_per_uom'],1)\n",
    "\n",
    "    median_price_by_uom_info_sku['eaches_per_uom_value_allowed'] = 0\n",
    "    median_price_by_uom_info_sku.loc[(median_price_by_uom_info_sku['percent_difference_from_median']>percent_difference_median_lower)&(median_price_by_uom_info_sku['percent_difference_from_median']<percent_difference_median_higher), 'eaches_per_uom_value_allowed'] = 1\n",
    "\n",
    "    # Export dataset\n",
    "    median_price_by_uom_info_sku.to_csv(csv_file_name+' - median_price_by_uom_info_sku '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "    # Create dataframe of correct/allowable eaches per uom values - keep any type/brand/product/each per uom combination that has a median price per each < 25% or >75% of the overall product median price per each\n",
    "    eaches_per_uom_allowed_sku = median_price_compare_sku[(median_price_compare_sku['percent_difference_from_median']<percent_difference_median_higher)&(median_price_compare_sku['percent_difference_from_median']>percent_difference_median_lower)]\n",
    "    eaches_per_uom_allowed_sku = eaches_per_uom_allowed_sku[['eaches_per_uom', 'count_uom']]\n",
    "    eaches_per_uom_allowed_sku = eaches_per_uom_allowed_sku.astype(int)\n",
    "    eaches_per_uom_allowed_sku.rename(columns={'eaches_per_uom': 'max_e_per_uom', 'count_uom': 'count_max_e_per_uom'}, inplace=True)\n",
    "\n",
    "    # Merge datasets\n",
    "    all_eaches_per_uom_allowed_sku = pd.merge(median_price_by_sku, eaches_per_uom_allowed_sku, how='outer', left_index=True, right_index=True)\n",
    "    all_eaches_per_uom_allowed_sku['additional_uom_value_1'] = np.nan ###edit v4\n",
    "    all_eaches_per_uom_allowed_sku['additional_uom_value_2'] = np.nan ###edit v4\n",
    "    all_eaches_per_uom_allowed_sku['additional_uom_value_3'] = np.nan ###edit v4\n",
    "    all_eaches_per_uom_allowed_sku['additional_uom_value_4'] = np.nan ###edit v4\n",
    "\n",
    "    # Delete multiple multiple e/uom values for type/brand/product combinations - keep most frequent\n",
    "    all_eaches_per_uom_allowed_sku = all_eaches_per_uom_allowed_sku.reset_index()\n",
    "    all_eaches_per_uom_allowed_sku = all_eaches_per_uom_allowed_sku.sort_values(by = ['matched_catalog_number_stripped', 'count_max_e_per_uom'], ascending = False)\n",
    "    all_eaches_per_uom_allowed_sku = all_eaches_per_uom_allowed_sku.drop_duplicates(['matched_catalog_number_stripped'], keep ='first')\n",
    "    all_eaches_per_uom_allowed_sku = all_eaches_per_uom_allowed_sku[['matched_catalog_number_stripped', 'median_price_sku', 'count_sku', 'min_e_per_uom', 'max_e_per_uom', 'count_max_e_per_uom', 'additional_uom_value_1', 'additional_uom_value_2', 'additional_uom_value_3', 'additional_uom_value_4']] ###edit v4\n",
    "    #all_eaches_per_uom_allowed_sku['median_price_sku'] = round(all_eaches_per_uom_allowed_sku['median_price_sku'], 2) ###edit v2\n",
    "\n",
    "    # Set index\n",
    "    all_eaches_per_uom_allowed_sku = all_eaches_per_uom_allowed_sku.set_index(['matched_catalog_number_stripped'])\n",
    "\n",
    "    # Create list of all eaches per uom that show up for each group/type/brand/product, and the count for each\n",
    "    all_eaches_per_uom_sku = benchmarking_data_sku.groupby(['matched_catalog_number_stripped', 'eaches_per_uom']).agg({'price_per_each':{'count_uom': 'count'}})\n",
    "    all_eaches_per_uom_sku.columns = all_eaches_per_uom_sku.columns.droplevel(0)\n",
    "    all_eaches_per_uom_sku.reset_index(['eaches_per_uom'], inplace=True)\n",
    "    all_eaches_per_uom_sku = all_eaches_per_uom_sku[all_eaches_per_uom_sku['eaches_per_uom']>0]\n",
    "    all_eaches_per_uom_sku_int = all_eaches_per_uom_sku.astype(int)\n",
    "    all_eaches_per_uom_sku_str = all_eaches_per_uom_sku_int.astype(str)\n",
    "    all_eaches_per_uom_sku_str['e_per_uom:count'] = all_eaches_per_uom_sku_str['eaches_per_uom'] + ':' + all_eaches_per_uom_sku_str['count_uom']\n",
    "    all_eaches_per_uom_sku_str = all_eaches_per_uom_sku_str.reset_index()\n",
    "    all_eaches_per_uom_sku_str = all_eaches_per_uom_sku_str[['matched_catalog_number_stripped', 'e_per_uom:count']]\n",
    "    all_eaches_per_uom_sku_list = all_eaches_per_uom_sku_str.groupby(['matched_catalog_number_stripped'])['e_per_uom:count'].apply(list)\n",
    "    all_eaches_per_uom_sku_list = all_eaches_per_uom_sku_list.to_frame()\n",
    "\n",
    "    # Merge\n",
    "    eaches_per_uom_info_sku = pd.merge(all_eaches_per_uom_allowed_sku, all_eaches_per_uom_sku_list, how='outer', left_index=True, right_index=True)\n",
    "    eaches_per_uom_info_sku = eaches_per_uom_info_sku.drop('count_max_e_per_uom', 1) ###edit v4\n",
    "    eaches_per_uom_info_sku['e_per_uom:count']\n",
    "\n",
    "    # Export dataset\n",
    "    eaches_per_uom_info_sku.to_csv(csv_file_name+' - eaches_per_uom_info_sku '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.csv', encoding = \"ISO-8859-1\" )\n",
    "\n",
    "else: \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Check and edit output in the \"csv_file_name - eaches_per_uom_info\" csv files\n",
    "1. Look for data issues - if there are any, fix po terms in the platform, re-run benchmarking, start script from beginning:\n",
    " - Products or skus where the median price (**median_price_product** & **median_price_sku**) is significantly different than other products or skus for that type.\n",
    " - Products with a high count of incorrect e/uom values (**e_per_uom:count column**)\n",
    "2. If there are no data issues, check for incorrect or missing max_e_per_uom values. \n",
    " - Look at the **max_e_per_uom** column to find values that should not be there.\n",
    " - Use the **e_per_uom:count column** (has all e/uom values seen in the benchmarking file for each product, and a total count of times that e/uom value showed up) to find values that are missing from the max_e_per_uom column\n",
    " - Use the **\"csv_file_name - median_price_by_uom_info\"** csv file for reference, which provides the median price for each uom value, and the percent difference from the overall median.\n",
    " - Change or delete any **max_e_per_uom** value if required, and save csv. Any changes to the max_e_per_uom column will be updated in the script.\n",
    " - (**NEW**) Add any values to the additional uom values columns, and save csv. Any values added will be allowed and used to fix data in addition to e/uom = 1 and e/uom = max."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Run section (fix e/uom)\n",
    "\n",
    "* Look at how many rows were affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################### SECTION: FIX E/UOM ####################################################\n",
    "\n",
    "##################################### TYPE LEVEL DATA #####################################\n",
    "\n",
    "if len(benchmarking_data_type) > 0:\n",
    "    # Read in fixed all_eaches_per_uom_allowed file\n",
    "    all_eaches_per_uom_allowed_type = pd.read_csv(csv_file_name+' - eaches_per_uom_info_type '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.csv', encoding = \"ISO-8859-1\")\n",
    "    all_eaches_per_uom_allowed_type = all_eaches_per_uom_allowed_type.sort_values(by=['groupname', 'typename', 'brand', 'product']).set_index(['groupname', 'typename', 'brand', 'product'])\n",
    "    all_eaches_per_uom_allowed_type = all_eaches_per_uom_allowed_type[['median_price_product','min_e_per_uom', 'max_e_per_uom', 'additional_uom_value_1', 'additional_uom_value_2', 'additional_uom_value_3', 'additional_uom_value_4']] ###edit v4\n",
    "    # Merge datasets for all info by product; create new row with initial eaches per uom value\n",
    "    data_with_uom_info_type = pd.merge(benchmarking_data_type_indexed, all_eaches_per_uom_allowed_type, how='left', left_index=True, right_index=True).reset_index()\n",
    "    data_with_uom_info_type['eaches_per_uom_initial'] = data_with_uom_info_type['eaches_per_uom']\n",
    "\n",
    "    # For all rows where max_e_per_uom is null and e/uom != 1, set e/uom to 1 and recalculate price_per_each\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['max_e_per_uom'].isnull())&(data_with_uom_info_type['eaches_per_uom'] !=1),'eaches_per_uom'] = 1\n",
    "\n",
    "    # For all rows where max_e_per_uom is not null, calculate multitude of difference comparing median_price_product by product with price_if_1 and price_if_max\n",
    "    data_with_uom_info_type['multiple_for_price_if_1'] = (data_with_uom_info_type['extended_cost']/(data_with_uom_info_type['units']*1))/data_with_uom_info_type['median_price_product']\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['max_e_per_uom'].notnull()), 'multiple_for_price_if_max'] = data_with_uom_info_type['median_price_product']/(data_with_uom_info_type['extended_cost']/(data_with_uom_info_type['units']*data_with_uom_info_type['max_e_per_uom']))\n",
    "\n",
    "    # For all rows where additional values are not null, calculate multitude of difference comparing median_price_product by product with additional values added ###edit v4 (change to function later to condense)\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['additional_uom_value_1'].notnull()), 'multiple_for_price_if_additional_1'] = data_with_uom_info_type['median_price_product']/(data_with_uom_info_type['extended_cost']/(data_with_uom_info_type['units']*data_with_uom_info_type['additional_uom_value_1']))\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['additional_uom_value_2'].notnull()), 'multiple_for_price_if_additional_2'] = data_with_uom_info_type['median_price_product']/(data_with_uom_info_type['extended_cost']/(data_with_uom_info_type['units']*data_with_uom_info_type['additional_uom_value_2']))\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['additional_uom_value_3'].notnull()), 'multiple_for_price_if_additional_3'] = data_with_uom_info_type['median_price_product']/(data_with_uom_info_type['extended_cost']/(data_with_uom_info_type['units']*data_with_uom_info_type['additional_uom_value_3']))\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['additional_uom_value_4'].notnull()), 'multiple_for_price_if_additional_4'] = data_with_uom_info_type['median_price_product']/(data_with_uom_info_type['extended_cost']/(data_with_uom_info_type['units']*data_with_uom_info_type['additional_uom_value_4']))\n",
    "\n",
    "    # Transform all multiple columns into absolute difference from 1 ###edit v4 (change to function later to condense)\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['multiple_for_price_if_1']<1), 'multiple_for_price_if_1_T'] = 1 - data_with_uom_info_type['multiple_for_price_if_1']\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['multiple_for_price_if_1']>=1), 'multiple_for_price_if_1_T'] = data_with_uom_info_type['multiple_for_price_if_1'] - 1\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['multiple_for_price_if_max']<1), 'multiple_for_price_if_max_T'] = 1 - data_with_uom_info_type['multiple_for_price_if_max']\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['multiple_for_price_if_max']>=1), 'multiple_for_price_if_max_T'] = data_with_uom_info_type['multiple_for_price_if_max'] - 1\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['multiple_for_price_if_additional_1']<1), 'multiple_for_price_if_additional_1_T'] = 1 - data_with_uom_info_type['multiple_for_price_if_additional_1']\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['multiple_for_price_if_additional_1']>=1), 'multiple_for_price_if_additional_1_T'] = data_with_uom_info_type['multiple_for_price_if_additional_1'] - 1\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['multiple_for_price_if_additional_2']<1), 'multiple_for_price_if_additional_2_T'] = 1 - data_with_uom_info_type['multiple_for_price_if_additional_2']\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['multiple_for_price_if_additional_2']>=1), 'multiple_for_price_if_additional_2_T'] = data_with_uom_info_type['multiple_for_price_if_additional_2'] - 1\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['multiple_for_price_if_additional_3']<1), 'multiple_for_price_if_additional_3_T'] = 1 - data_with_uom_info_type['multiple_for_price_if_additional_3']\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['multiple_for_price_if_additional_3']>=1), 'multiple_for_price_if_additional_3_T'] = data_with_uom_info_type['multiple_for_price_if_additional_3'] - 1\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['multiple_for_price_if_additional_4']<1), 'multiple_for_price_if_additional_4_T'] = 1 - data_with_uom_info_type['multiple_for_price_if_additional_4']\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['multiple_for_price_if_additional_4']>=1), 'multiple_for_price_if_additional_4_T'] = data_with_uom_info_type['multiple_for_price_if_additional_4'] - 1\n",
    "    data_with_uom_info_type[['groupname', 'typename', 'brand', 'product', 'provider_name', 'eaches_per_uom', 'price_per_each', 'multiple_for_price_if_1']]\n",
    "\n",
    "    # Find column with min difference, set e/uom to corresponding value ###edit v4 (change to function later to condense)\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['multiple_for_price_if_1_T'].notnull()), 'final_uom_value'] = data_with_uom_info_type[['multiple_for_price_if_1_T','multiple_for_price_if_max_T', 'multiple_for_price_if_additional_1_T', 'multiple_for_price_if_additional_2_T', 'multiple_for_price_if_additional_3_T', 'multiple_for_price_if_additional_4_T']].idxmin(axis=1)\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['multiple_for_price_if_1_T'].isnull()), 'final_uom_value'] = 'N/A'\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['final_uom_value'] == 'multiple_for_price_if_1_T'),'eaches_per_uom'] = data_with_uom_info_type['min_e_per_uom']\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['final_uom_value'] == 'multiple_for_price_if_max_T'),'eaches_per_uom'] = data_with_uom_info_type['max_e_per_uom']\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['final_uom_value'] == 'multiple_for_price_if_additional_1_T'),'eaches_per_uom'] = data_with_uom_info_type['additional_uom_value_1']\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['final_uom_value'] == 'multiple_for_price_if_additional_2_T'),'eaches_per_uom'] = data_with_uom_info_type['additional_uom_value_2']\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['final_uom_value'] == 'multiple_for_price_if_additional_3_T'),'eaches_per_uom'] = data_with_uom_info_type['additional_uom_value_3']\n",
    "    data_with_uom_info_type.loc[(data_with_uom_info_type['final_uom_value'] == 'multiple_for_price_if_additional_4_T'),'eaches_per_uom'] = data_with_uom_info_type['additional_uom_value_4']\n",
    "\n",
    "\n",
    "    # Recalculate price_per_each and total eaches\n",
    "    data_with_uom_info_type['price_per_each'] = data_with_uom_info_type['extended_cost']/(data_with_uom_info_type['units']*data_with_uom_info_type['eaches_per_uom']).where(data_with_uom_info_type['eaches_per_uom']>0)\n",
    "    data_with_uom_info_type['price_per_each'] = data_with_uom_info_type['price_per_each'].round(4)\n",
    "    data_with_uom_info_type['totaleaches'] = (data_with_uom_info_type['units']*data_with_uom_info_type['eaches_per_uom'])\n",
    "\n",
    "    # Drop created columns ###edit v4\n",
    "    data_with_uom_info_type = data_with_uom_info_type.drop(['median_price_product', 'min_e_per_uom', 'max_e_per_uom', 'additional_uom_value_1', 'additional_uom_value_2', 'additional_uom_value_3', 'additional_uom_value_4', \n",
    "                                                                  'multiple_for_price_if_1', 'multiple_for_price_if_max', 'multiple_for_price_if_additional_1', 'multiple_for_price_if_additional_2', 'multiple_for_price_if_additional_3', 'multiple_for_price_if_additional_4',\n",
    "                                                                  'multiple_for_price_if_1_T', 'multiple_for_price_if_max_T', 'multiple_for_price_if_additional_1_T', 'multiple_for_price_if_additional_2_T', 'multiple_for_price_if_additional_3_T', 'multiple_for_price_if_additional_4_T',\n",
    "                                                                  'final_uom_value'],1)\n",
    "    \n",
    "    # Print how many rows were affected\n",
    "    print(len(data_with_uom_info_type[(data_with_uom_info_type['eaches_per_uom'] != data_with_uom_info_type['eaches_per_uom_initial'])])\n",
    "    ,'rows were affected from type level data')\n",
    "    \n",
    "else: \n",
    "    pass\n",
    "\n",
    "##################################### SKU LEVEL DATA #####################################\n",
    "\n",
    "if len(benchmarking_data_sku) > 0:\n",
    "    # Read in fixed all_eaches_per_uom_allowed file\n",
    "    all_eaches_per_uom_allowed_sku = pd.read_csv(csv_file_name+' - eaches_per_uom_info_sku '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.csv', encoding = \"ISO-8859-1\")\n",
    "    all_eaches_per_uom_allowed_sku = all_eaches_per_uom_allowed_sku.sort_values(by=['matched_catalog_number_stripped']).set_index(['matched_catalog_number_stripped'])\n",
    "    all_eaches_per_uom_allowed_sku = all_eaches_per_uom_allowed_sku[['median_price_sku','min_e_per_uom', 'max_e_per_uom', 'additional_uom_value_1', 'additional_uom_value_2', 'additional_uom_value_3', 'additional_uom_value_4']] ###edit v4\n",
    "\n",
    "    # Merge datasets for all info by product; create new row with initial eaches per uom value\n",
    "    data_with_uom_info_sku = pd.merge(benchmarking_data_sku_indexed, all_eaches_per_uom_allowed_sku, how='left', left_index=True, right_index=True).reset_index()\n",
    "    data_with_uom_info_sku['eaches_per_uom_initial'] = data_with_uom_info_sku['eaches_per_uom']\n",
    "\n",
    "    # For all rows where max_e_per_uom is null and e/uom != 1, set e/uom to 1 and recalculate price_per_each\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['max_e_per_uom'].isnull())&(data_with_uom_info_sku['eaches_per_uom'] !=1),'eaches_per_uom'] = 1\n",
    "\n",
    "    # For all rows where max_e_per_uom is not null, calculate multitude of difference comparing median_price_product by product with price_if_1 and price_if_max\n",
    "    data_with_uom_info_sku['multiple_for_price_if_1'] = (data_with_uom_info_sku['extended_cost']/(data_with_uom_info_sku['units']*1))/data_with_uom_info_sku['median_price_sku']\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['max_e_per_uom'].notnull()), 'multiple_for_price_if_max'] = data_with_uom_info_sku['median_price_sku']/(data_with_uom_info_sku['extended_cost']/(data_with_uom_info_sku['units']*data_with_uom_info_sku['max_e_per_uom']))\n",
    "\n",
    "    # For all rows where additional values are not null, calculate multitude of difference comparing median_price_product by product with additional values added ###edit v4 (change to function later to condense)\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['additional_uom_value_1'].notnull()), 'multiple_for_price_if_additional_1'] = data_with_uom_info_sku['median_price_sku']/(data_with_uom_info_sku['extended_cost']/(data_with_uom_info_sku['units']*data_with_uom_info_sku['additional_uom_value_1']))\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['additional_uom_value_2'].notnull()), 'multiple_for_price_if_additional_2'] = data_with_uom_info_sku['median_price_sku']/(data_with_uom_info_sku['extended_cost']/(data_with_uom_info_sku['units']*data_with_uom_info_sku['additional_uom_value_2']))\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['additional_uom_value_3'].notnull()), 'multiple_for_price_if_additional_3'] = data_with_uom_info_sku['median_price_sku']/(data_with_uom_info_sku['extended_cost']/(data_with_uom_info_sku['units']*data_with_uom_info_sku['additional_uom_value_3']))\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['additional_uom_value_4'].notnull()), 'multiple_for_price_if_additional_4'] = data_with_uom_info_sku['median_price_sku']/(data_with_uom_info_sku['extended_cost']/(data_with_uom_info_sku['units']*data_with_uom_info_sku['additional_uom_value_4']))\n",
    "\n",
    "    # Transform all multiple columns into absolute difference from 1 ###edit v4 (change to function later to condense)\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['multiple_for_price_if_1']<1), 'multiple_for_price_if_1_T'] = 1 - data_with_uom_info_sku['multiple_for_price_if_1']\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['multiple_for_price_if_1']>=1), 'multiple_for_price_if_1_T'] = data_with_uom_info_sku['multiple_for_price_if_1'] - 1\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['multiple_for_price_if_max']<1), 'multiple_for_price_if_max_T'] = 1 - data_with_uom_info_sku['multiple_for_price_if_max']\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['multiple_for_price_if_max']>=1), 'multiple_for_price_if_max_T'] = data_with_uom_info_sku['multiple_for_price_if_max'] - 1\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['multiple_for_price_if_additional_1']<1), 'multiple_for_price_if_additional_1_T'] = 1 - data_with_uom_info_sku['multiple_for_price_if_additional_1']\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['multiple_for_price_if_additional_1']>=1), 'multiple_for_price_if_additional_1_T'] = data_with_uom_info_sku['multiple_for_price_if_additional_1'] - 1\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['multiple_for_price_if_additional_2']<1), 'multiple_for_price_if_additional_2_T'] = 1 - data_with_uom_info_sku['multiple_for_price_if_additional_2']\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['multiple_for_price_if_additional_2']>=1), 'multiple_for_price_if_additional_2_T'] = data_with_uom_info_sku['multiple_for_price_if_additional_2'] - 1\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['multiple_for_price_if_additional_3']<1), 'multiple_for_price_if_additional_3_T'] = 1 - data_with_uom_info_sku['multiple_for_price_if_additional_3']\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['multiple_for_price_if_additional_3']>=1), 'multiple_for_price_if_additional_3_T'] = data_with_uom_info_sku['multiple_for_price_if_additional_3'] - 1\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['multiple_for_price_if_additional_4']<1), 'multiple_for_price_if_additional_4_T'] = 1 - data_with_uom_info_sku['multiple_for_price_if_additional_4']\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['multiple_for_price_if_additional_4']>=1), 'multiple_for_price_if_additional_4_T'] = data_with_uom_info_sku['multiple_for_price_if_additional_4'] - 1\n",
    "\n",
    "    # Find column with min difference, set e/uom to corresponding value ###edit v4 (change to function later to condense)\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['multiple_for_price_if_1_T'].notnull()), 'final_uom_value'] = data_with_uom_info_sku[['multiple_for_price_if_1_T','multiple_for_price_if_max_T', 'multiple_for_price_if_additional_1_T', 'multiple_for_price_if_additional_2_T', 'multiple_for_price_if_additional_3_T', 'multiple_for_price_if_additional_4_T']].idxmin(axis=1)\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['multiple_for_price_if_1_T'].isnull()), 'final_uom_value'] = 'N/A'\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['final_uom_value'] == 'multiple_for_price_if_1_T'),'eaches_per_uom'] = data_with_uom_info_sku['min_e_per_uom']\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['final_uom_value'] == 'multiple_for_price_if_max_T'),'eaches_per_uom'] = data_with_uom_info_sku['max_e_per_uom']\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['final_uom_value'] == 'multiple_for_price_if_additional_1_T'),'eaches_per_uom'] = data_with_uom_info_sku['additional_uom_value_1']\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['final_uom_value'] == 'multiple_for_price_if_additional_2_T'),'eaches_per_uom'] = data_with_uom_info_sku['additional_uom_value_2']\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['final_uom_value'] == 'multiple_for_price_if_additional_3_T'),'eaches_per_uom'] = data_with_uom_info_sku['additional_uom_value_3']\n",
    "    data_with_uom_info_sku.loc[(data_with_uom_info_sku['final_uom_value'] == 'multiple_for_price_if_additional_4_T'),'eaches_per_uom'] = data_with_uom_info_sku['additional_uom_value_4']\n",
    "\n",
    "    # Recalculate price_per_each and total eaches\n",
    "    data_with_uom_info_sku['price_per_each'] = data_with_uom_info_sku['extended_cost']/(data_with_uom_info_sku['units']*data_with_uom_info_sku['eaches_per_uom']).where(data_with_uom_info_sku['eaches_per_uom']>0)\n",
    "    data_with_uom_info_sku['price_per_each'] = data_with_uom_info_sku['price_per_each'].round(4)\n",
    "    data_with_uom_info_sku['totaleaches'] = (data_with_uom_info_sku['units']*data_with_uom_info_sku['eaches_per_uom'])\n",
    "\n",
    "    # Drop created columns ###edit v4\n",
    "    data_with_uom_info_sku = data_with_uom_info_sku.drop(['median_price_sku', 'min_e_per_uom', 'max_e_per_uom', 'additional_uom_value_1', 'additional_uom_value_2', 'additional_uom_value_3', 'additional_uom_value_4', \n",
    "                                                                  'multiple_for_price_if_1', 'multiple_for_price_if_max', 'multiple_for_price_if_additional_1', 'multiple_for_price_if_additional_2', 'multiple_for_price_if_additional_3', 'multiple_for_price_if_additional_4',\n",
    "                                                                  'multiple_for_price_if_1_T', 'multiple_for_price_if_max_T', 'multiple_for_price_if_additional_1_T', 'multiple_for_price_if_additional_2_T', 'multiple_for_price_if_additional_3_T', 'multiple_for_price_if_additional_4_T',\n",
    "                                                                  'final_uom_value'],1)\n",
    "\n",
    "    # Recalculate price_per_each and total eaches\n",
    "    data_with_uom_info_sku['price_per_each'] = data_with_uom_info_sku['extended_cost']/(data_with_uom_info_sku['units']*data_with_uom_info_sku['eaches_per_uom']).where(data_with_uom_info_sku['eaches_per_uom']>0)\n",
    "    data_with_uom_info_sku['price_per_each'] = data_with_uom_info_sku['price_per_each'].round(4)\n",
    "    data_with_uom_info_sku['totaleaches'] = (data_with_uom_info_sku['units']*data_with_uom_info_sku['eaches_per_uom'])\n",
    "\n",
    "    # Print how many rows were affected\n",
    "    print(len(data_with_uom_info_sku[(data_with_uom_info_sku['eaches_per_uom'] != data_with_uom_info_sku['eaches_per_uom_initial'])])\n",
    "    ,'rows were affected from sku level data')\n",
    "    \n",
    "else: \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Run section (flag outliers)\n",
    "\n",
    "* Look at how many outliers were flagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###################################### SECTION: FLAG OUTLIERS #################################################\n",
    "\n",
    "##################################### TYPE LEVEL DATA #####################################\n",
    "\n",
    "if len(benchmarking_data_type) > 0:\n",
    "    #Cutoff parameters\n",
    "    cutoff_1_from_median_type = .1 \n",
    "    cutoff_2_from_25th_type = .3\n",
    "    cutoff_3_from_10th_type = .6 \n",
    "    cutoff_3_from_75th_type = 3 \n",
    "\n",
    "    # Calculate initial median by type\n",
    "    cutoff_1_type = data_with_uom_info_type.groupby(['groupname', 'typename']).agg({'price_per_each':{'type_median_initial': percentile(50)}})\n",
    "    cutoff_1_type.columns = cutoff_1_type.columns.droplevel(0)\n",
    "\n",
    "    # Index and merge\n",
    "    data_with_uom_info_type_indexed = data_with_uom_info_type[(data_with_uom_info_type['typename'].notnull())&(data_with_uom_info_type['product'].notnull())&(data_with_uom_info_type['brand'].notnull())].sort_values(by=['groupname', 'typename', 'brand', 'product']).set_index(['groupname', 'typename'])\n",
    "    data_cutoff_1_type = pd.merge(data_with_uom_info_type_indexed, cutoff_1_type, how = 'left', left_index=True, right_index=True).reset_index()\n",
    "\n",
    "    # Create outlier flag, initially set to 0. Set outlier flag to 1 for any price_per_each < .1*median price for type\n",
    "    data_cutoff_1_type['outlier_flag'] = 0\n",
    "    data_cutoff_1_type.loc[(data_cutoff_1_type['price_per_each']/data_cutoff_1_type['type_median_initial']<cutoff_1_from_median_type), 'outlier_flag'] = 1\n",
    "\n",
    "    # Calculate 25th percentile by type after initial cutoff\n",
    "    cutoff_2_type = data_cutoff_1_type[data_cutoff_1_type['outlier_flag']==0].groupby(['groupname', 'typename']).agg({'price_per_each':{'type_25th': percentile(25)}})\n",
    "    cutoff_2_type.columns = cutoff_2_type.columns.droplevel(0)\n",
    "\n",
    "    # Index and merge; Set outlier flag to 1 for any price_per_each < .25*25th percentile price for type\n",
    "    data_cutoff_1_type_indexed = data_cutoff_1_type[(data_cutoff_1_type['typename'].notnull())&(data_cutoff_1_type['product'].notnull())&(data_cutoff_1_type['brand'].notnull())].sort_values(by=['groupname', 'typename', 'brand', 'product']).set_index(['groupname', 'typename'])\n",
    "    data_cutoff_2_type = pd.merge(data_cutoff_1_type_indexed, cutoff_2_type, how = 'left', left_index=True, right_index=True).reset_index()\n",
    "    data_cutoff_2_type.loc[(data_cutoff_2_type['price_per_each']/data_cutoff_2_type['type_25th']<cutoff_2_from_25th_type), 'outlier_flag'] = 1\n",
    "\n",
    "    # Calculate 10th and 75th percentile by type\n",
    "    cutoff_3_type = data_cutoff_2_type[data_cutoff_2_type['outlier_flag']==0].groupby(['groupname', 'typename']).agg({'price_per_each':{'type_10th': percentile(10), 'type_75th': percentile(75)}})\n",
    "    cutoff_3_type.columns = cutoff_3_type.columns.droplevel(0)\n",
    "\n",
    "    # Index and merge; Set outlier flag to 1 for any price_per_each < .25*25th percentile price for type\n",
    "    data_cutoff_2_type_indexed = data_cutoff_2_type[(data_cutoff_2_type['typename'].notnull())&(data_cutoff_2_type['product'].notnull())&(data_cutoff_2_type['brand'].notnull())].sort_values(by=['groupname', 'typename', 'brand', 'product']).set_index(['groupname', 'typename'])\n",
    "    data_cutoff_3_type = pd.merge(data_cutoff_2_type_indexed, cutoff_3_type, how = 'left', left_index=True, right_index=True).reset_index()\n",
    "    data_cutoff_3_type.loc[(data_cutoff_3_type['price_per_each']/data_cutoff_3_type['type_10th']<cutoff_3_from_10th_type) | (data_cutoff_3_type['price_per_each']/data_cutoff_3_type['type_75th']>cutoff_3_from_75th_type), 'outlier_flag'] = 1\n",
    "    final_data_type = data_cutoff_3_type.sort_values('price_per_each').drop(['type_median_initial', 'type_25th', 'type_75th', 'type_10th'],1).fillna('NULL') ###edit v4\n",
    "\n",
    "    # Export dataset\n",
    "    final_data_type.to_csv(csv_file_name+' - edited_type '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.csv', index=False, encoding = \"ISO-8859-1\")\n",
    "\n",
    "    # Print how many rows were affected\n",
    "    if CLEAN_UP_TYPE == 'opportunity':\n",
    "        print(len(final_data_type[(final_data_type['outlier_flag']==1)&(final_data_type['exclude_from_benchmarking']==False)])\n",
    "        ,'rows were flagged as outliers from type level data')\n",
    "    else:\n",
    "        print(len(final_data_type[(final_data_type['outlier_flag']==1)&(final_data_type['exclude_from_roi']==0)])\n",
    "        ,'rows were flagged as outliers from type level data')\n",
    "\n",
    "else: \n",
    "    pass\n",
    "\n",
    "##################################### SKU LEVEL DATA #####################################\n",
    "\n",
    "if len(benchmarking_data_sku) > 0:\n",
    "    #Cutoff parameters\n",
    "    cutoff_1_from_median_sku = .5\n",
    "    cutoff_2_from_25th_sku = .6\n",
    "    cutoff_3_from_10th_sku = .6\n",
    "    cutoff_3_from_75th_sku = 1.5\n",
    "\n",
    "    # Calculate initial median by type\n",
    "    cutoff_1_sku = data_with_uom_info_sku.groupby(['matched_catalog_number_stripped']).agg({'price_per_each':{'sku_median_initial': percentile(50)}})\n",
    "    cutoff_1_sku.columns = cutoff_1_sku.columns.droplevel(0)\n",
    "\n",
    "    # Index and merge\n",
    "    data_with_uom_info_sku_indexed = data_with_uom_info_sku[(data_with_uom_info_sku['typename'].notnull())&(data_with_uom_info_sku['product'].notnull())&(data_with_uom_info_sku['brand'].notnull())].sort_values(by=['matched_catalog_number_stripped']).set_index(['matched_catalog_number_stripped'])\n",
    "    data_cutoff_1_sku = pd.merge(data_with_uom_info_sku_indexed, cutoff_1_sku, how = 'left', left_index=True, right_index=True).reset_index()\n",
    "\n",
    "    # Create outlier flag, initially set to 0. Set outlier flag to 1 for any price_per_each < .1*median price for type\n",
    "    data_cutoff_1_sku['outlier_flag'] = 0\n",
    "    data_cutoff_1_sku.loc[(data_cutoff_1_sku['price_per_each']/data_cutoff_1_sku['sku_median_initial']<cutoff_1_from_median_sku), 'outlier_flag'] = 1\n",
    "\n",
    "    # Calculate 25th percentile by type after initial cutoff\n",
    "    cutoff_2_sku = data_cutoff_1_sku[data_cutoff_1_sku['outlier_flag']==0].groupby(['matched_catalog_number_stripped']).agg({'price_per_each':{'sku_25th': percentile(25)}})\n",
    "    cutoff_2_sku.columns = cutoff_2_sku.columns.droplevel(0)\n",
    "\n",
    "    # Index and merge; Set outlier flag to 1 for any price_per_each < .25*25th percentile price for type\n",
    "    data_cutoff_1_sku_indexed = data_cutoff_1_sku[(data_cutoff_1_sku['typename'].notnull())&(data_cutoff_1_sku['product'].notnull())&(data_cutoff_1_sku['brand'].notnull())].sort_values(by=['matched_catalog_number_stripped']).set_index(['matched_catalog_number_stripped'])\n",
    "    data_cutoff_2_sku = pd.merge(data_cutoff_1_sku_indexed, cutoff_2_sku, how = 'left', left_index=True, right_index=True).reset_index()\n",
    "    data_cutoff_2_sku.loc[(data_cutoff_2_sku['price_per_each']/data_cutoff_2_sku['sku_25th']<cutoff_2_from_25th_sku), 'outlier_flag'] = 1\n",
    "\n",
    "    # Calculate 10th and 75th percentile by type\n",
    "    cutoff_3_sku = data_cutoff_2_sku[data_cutoff_2_sku['outlier_flag']==0].groupby(['matched_catalog_number_stripped']).agg({'price_per_each':{'sku_10th': percentile(10), 'sku_75th': percentile(75)}})\n",
    "    cutoff_3_sku.columns = cutoff_3_sku.columns.droplevel(0)\n",
    "\n",
    "    # Index and merge; Set outlier flag to 1 for any price_per_each < .25*25th percentile price for type\n",
    "    data_cutoff_2_sku_indexed = data_cutoff_2_sku[(data_cutoff_2_sku['typename'].notnull())&(data_cutoff_2_sku['product'].notnull())&(data_cutoff_2_sku['brand'].notnull())].sort_values(by=['matched_catalog_number_stripped']).set_index(['matched_catalog_number_stripped'])\n",
    "    data_cutoff_3_sku = pd.merge(data_cutoff_2_sku_indexed, cutoff_3_sku, how = 'left', left_index=True, right_index=True).reset_index()\n",
    "    data_cutoff_3_sku.loc[(data_cutoff_3_sku['price_per_each']/data_cutoff_3_sku['sku_10th']<cutoff_3_from_10th_sku) | (data_cutoff_3_sku['price_per_each']/data_cutoff_3_sku['sku_75th']>cutoff_3_from_75th_sku), 'outlier_flag'] = 1\n",
    "    final_data_sku = data_cutoff_3_sku.sort_values('price_per_each').drop(['sku_median_initial', 'sku_25th', 'sku_75th', 'sku_10th'],1).fillna('NULL') ###edit v4\n",
    "\n",
    "    # Export dataset\n",
    "    final_data_sku.to_csv(csv_file_name+' - edited_sku '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.csv', index=False, encoding = \"ISO-8859-1\")\n",
    "\n",
    "    # Print how many rows were affected\n",
    "    if CLEAN_UP_TYPE == 'opportunity':\n",
    "        print(len(final_data_sku[(final_data_sku['outlier_flag']==1)&(final_data_sku['exclude_from_benchmarking']==False)])\n",
    "        ,'rows were flagged as outliers from sku level data')\n",
    "    else: \n",
    "        print(len(final_data_sku[(final_data_sku['outlier_flag']==1)&(final_data_sku['exclude_from_roi']==0)])\n",
    "        ,'rows were flagged as outliers from sku level data')\n",
    "\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Check printed price distributions after fixing e/uom and excluding outliers\n",
    "\n",
    "1. Final distribution by **type** and **number of low and high outliers** (large number of outliers may indicate incorrect po terms pulled in)\n",
    "2. **Types** with high variability from min to 25th\n",
    "3. **Products** with high variability from min to 25th\n",
    "4. **Providers** with high variability from min to 25th or 25th to 75th within a particular type/product\n",
    "5. **SKU's** with high variability from min to 25th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Final distribution by type and number of outliers\n",
    "\n",
    "if len(benchmarking_data_type) > 0:\n",
    "    low_outliers_type = data_cutoff_3_type[(data_cutoff_3_type['outlier_flag']==1)&(data_cutoff_3_type['price_per_each']<data_cutoff_3_type['type_10th'])].groupby(['groupname', 'typename']).agg({'price_per_each':{'count_low_outliers': 'count'}})\n",
    "    low_outliers_type.columns = low_outliers_type.columns.droplevel(0)\n",
    "    high_outliers_type = data_cutoff_3_type[(data_cutoff_3_type['outlier_flag']==1)&(data_cutoff_3_type['price_per_each']>data_cutoff_3_type['type_75th'])].groupby(['groupname', 'typename']).agg({'price_per_each':{'count_high_outliers': 'count'}})\n",
    "    high_outliers_type.columns = high_outliers_type.columns.droplevel(0)\n",
    "    final_distribution_type = data_cutoff_3_type[data_cutoff_3_type['outlier_flag']==0].groupby(['groupname', 'typename']).agg({'price_per_each':{'count_type': 'count', 'median': 'median', '10th': percentile(10), '25th': percentile(25), '75th': percentile(75), 'min': 'min', 'max': 'max'}})\n",
    "    final_distribution_type.columns = final_distribution_type.columns.droplevel(0)\n",
    "    final_distribution_type = pd.merge(final_distribution_type, low_outliers_type, how = 'left', left_index=True, right_index=True)\n",
    "    final_distribution_type = pd.merge(final_distribution_type, high_outliers_type, how = 'left', left_index=True, right_index=True)\n",
    "    final_distribution_type = final_distribution_type[['count_type', 'count_low_outliers', 'count_high_outliers', 'min', '10th', '25th', 'median', '75th', 'max']]\n",
    "else: \n",
    "    print(\"no type data\")\n",
    "\n",
    "final_distribution_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. Types with high variability from min to 25th\n",
    "if len(benchmarking_data_type) > 0:\n",
    "    final_distribution_type_variable = final_distribution_type[final_distribution_type['min']<final_distribution_type['25th']*.5]\n",
    "else: \n",
    "    print(\"no type data\")\n",
    "\n",
    "final_distribution_type_variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. Products with high variability from min to 25th\n",
    "\n",
    "if len(benchmarking_data_type) > 0:\n",
    "    final_distribution_product = data_cutoff_3_type[data_cutoff_3_type['outlier_flag']==0].groupby(['groupname', 'typename', 'brand', 'product']).agg({'price_per_each':{'count_product': 'count', 'median': 'median', '10th': percentile(10), '25th': percentile(25), '75th': percentile(75), 'min': 'min', 'max': 'max'}})\n",
    "    final_distribution_product.columns = final_distribution_product.columns.droplevel(0)\n",
    "    final_distribution_product = final_distribution_product[['min', '10th', '25th', 'median', '75th', 'max']]\n",
    "\n",
    "    final_distribution_product_variable = final_distribution_product[final_distribution_product['min']<final_distribution_product['25th']*.5]\n",
    "else: \n",
    "    print(\"no type data\")\n",
    "\n",
    "final_distribution_product_variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4. Providers with high variability from min to 25th or 25th to 75th within a particular type/product\n",
    "\n",
    "if len(benchmarking_data_type) > 0:\n",
    "    final_distribution_provider_product = data_cutoff_3_type[data_cutoff_3_type['outlier_flag']==0].groupby(['groupname', 'typename', 'brand', 'product', 'provider_name']).agg({'price_per_each':{'count': 'count', 'median': 'median', '25th': percentile(25), '75th': percentile(75), 'min': 'min', 'max': 'max'}})\n",
    "    final_distribution_provider_product.columns = final_distribution_provider_product.columns.droplevel(0)\n",
    "    final_distribution_provider_product_variable = final_distribution_provider_product[((final_distribution_provider_product['75th']/final_distribution_provider_product['25th']) >2) | ((final_distribution_provider_product['25th']/final_distribution_provider_product['min']) >2)]\n",
    "    final_distribution_provider_product_variable = final_distribution_provider_product_variable[['count','min', '25th', 'median', '75th', 'max']]\n",
    "else: \n",
    "    print(\"no type data\")\n",
    "\n",
    "final_distribution_provider_product_variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5. SKU's with high variability from the min to the median\n",
    "\n",
    "if len(benchmarking_data_sku) > 0:\n",
    "    final_distribution_sku = data_cutoff_3_sku[data_cutoff_3_sku['outlier_flag']==0].groupby(['matched_catalog_number_stripped']).agg({'price_per_each':{'count_type': 'count', 'median': 'median', '10th': percentile(10), '25th': percentile(25), '75th': percentile(75), 'min': 'min', 'max': 'max'}})\n",
    "    final_distribution_sku.columns = final_distribution_sku.columns.droplevel(0)\n",
    "    final_distribution_sku = final_distribution_sku[['min', '10th', '25th', 'median', '75th', 'max']]\n",
    "\n",
    "    final_distribution_sku_variable = final_distribution_sku[final_distribution_sku['min']<final_distribution_sku['median']*.5]\n",
    "else: \n",
    "    print(\"no sku data\")\n",
    "    \n",
    "final_distribution_sku_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Look for any remaining issues with outliers or e/uom values, fix in the \"csv_file_name - edited\" csv file\n",
    "\n",
    "* Update the **\"outlier_flagged\"** or **\"each_per_uom\"** & **\"price_per_each\"** column values. Any changes to the csv will be updated in the script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Export final datasets: benchmarking, distribution by product, distribution by type\n",
    " - Verify distributions look good before running update statements. Can make changes to **\"csv_file_name - edited\"** csv file and run this section again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import final data and merge type and sku level data if there is both\n",
    "if len(benchmarking_data_type) == 0:\n",
    "    final_data = pd.read_csv(csv_file_name+' - edited_sku '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.csv', encoding = \"ISO-8859-1\", dtype = {'eaches_per_uom': np.int})\n",
    "    print(\"sku data only\")\n",
    "elif len(benchmarking_data_sku) == 0:\n",
    "    final_data = pd.read_csv(csv_file_name+' - edited_type '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.csv', encoding = \"ISO-8859-1\", dtype = {'eaches_per_uom': np.int})\n",
    "    print(\"type data only\")\n",
    "else:\n",
    "    final_data_sku = pd.read_csv(csv_file_name+' - edited_sku '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.csv', encoding = \"ISO-8859-1\", dtype = {'eaches_per_uom': np.int})\n",
    "    final_data_type = pd.read_csv(csv_file_name+' - edited_type '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.csv', encoding = \"ISO-8859-1\", dtype = {'eaches_per_uom': np.int})\n",
    "    final_data = pd.concat([final_data_type, final_data_sku], axis=0)\n",
    "    print(\"both sku and type data\")\n",
    "\n",
    "if CLEAN_UP_TYPE == 'opportunity':\n",
    "    final_data = final_data[['groupname', 'typename', 'brand', 'product', 'matched_catalog_number_stripped',\n",
    "                        'provider_name', 'facility_name', 'type', 'region', 'bed_size', 'item_number', 'item_description',\n",
    "                        'units', 'unit_of_measure', 'eaches_per_uom',  \n",
    "                         'price_per_each', 'unit_price', \n",
    "                         'extended_cost', 'totaleaches',\n",
    "                         'id', 'manufacturer_name', 'manufacturer_catalog_number', 'manufacturer_catalog_number_stripped',\n",
    "                         'vendor_name', 'vendor_catalog_number', 'vendor_catalog_number_stripped', \n",
    "                         'exclude_from_benchmarking', 'po_number', \n",
    "                         'po_line_number', 'po_date',  'group_type_id', 'group_id', 'product_id', \n",
    "                         'provider_id', 'eaches_per_uom_initial', 'outlier_flag','scrutinized_opp_level',\n",
    "                         'scrutinized_dashboard_level']]\n",
    "else:\n",
    "    final_data = final_data[['groupname', 'typename', 'brand', 'product', 'matched_catalog_number_stripped',\n",
    "                        'provider_name', 'facility_name', 'type', 'region', 'bed_size', 'item_number',\n",
    "                        'units', 'unit_of_measure', 'eaches_per_uom',  \n",
    "                         'price_per_each', 'unit_price', \n",
    "                         'extended_cost', 'totaleaches',\n",
    "                         'id', 'manufacturer_name', 'manufacturer_catalog_number', 'manufacturer_catalog_number_stripped',\n",
    "                         'vendor_name', 'vendor_catalog_number', 'vendor_catalog_number_stripped', 'po_number', \n",
    "                         'po_line_number', 'po_date',  'group_type_id', 'group_id', 'product_id', \n",
    "                         'provider_id', 'eaches_per_uom_initial', 'outlier_flag','scrutinized_opp_level',\n",
    "                         'scrutinized_dashboard_level','exclude_from_roi']]\n",
    "\n",
    "\n",
    "# Recalculate price\n",
    "final_data['price_per_each'] = final_data['extended_cost']/(final_data['units']*final_data['eaches_per_uom'])\n",
    "\n",
    "# Create dataset with all ids where e/uom changed\n",
    "final_data['e_per_uom_changed'] = 0\n",
    "final_data.loc[(final_data['eaches_per_uom'] != final_data['eaches_per_uom_initial']), 'e_per_uom_changed'] = 1\n",
    "if UPDATE_ALL_EUOM == 'T':\n",
    "    e_per_uom_changed = final_data[(final_data['e_per_uom_changed']==1)]\n",
    "else:\n",
    "    e_per_uom_changed = final_data[(final_data['e_per_uom_changed']==1)&(final_data['scrutinized_opp_level']==0)]\n",
    "\n",
    "# Create dataset with all ids where outlier_flag = 1 and row is not excluded from benchmarking\n",
    "if CLEAN_UP_TYPE == 'opportunity':\n",
    "    new_outliers = final_data[(final_data['outlier_flag']==1)&(final_data['exclude_from_benchmarking']==False)]\n",
    "else:\n",
    "    new_outliers = final_data[(final_data['outlier_flag']==1)&(final_data['exclude_from_roi']==0)]\n",
    "\n",
    "# Create dataset with all ids where outlier_flag = 0 and row IS currently excluded from benchmarking\n",
    "if CLEAN_UP_TYPE == 'opportunity':\n",
    "    no_longer_outliers = final_data[(final_data['outlier_flag']==0)&(final_data['exclude_from_benchmarking']==True)]\n",
    "else:\n",
    "    no_longer_outliers = final_data[(final_data['outlier_flag']==0)&(final_data['exclude_from_roi']==1)]\n",
    "    \n",
    "# Create dataset with all ids that have never been cleaned up before\n",
    "insert_clean_up_tracker = final_data[(final_data['scrutinized_opp_level']==0)&(final_data['scrutinized_dashboard_level']==0)]\n",
    "\n",
    "# Final benchmarking dataset with outliers removed\n",
    "final_data_outliers_removed = final_data[final_data['outlier_flag']==0]\n",
    "final_data_outliers_removed.to_csv(csv_file_name+' - final_benchmarking_data_outliers_removed.csv', index=False, encoding = \"ISO-8859-1\")\n",
    "\n",
    "# Final distribution by product\n",
    "final_distribution_product = final_data[final_data['outlier_flag']==0].groupby(['groupname', 'typename', 'brand', 'product']).agg({'price_per_each':{'count_product': 'count', 'median': 'median', '10th': percentile(10), '25th': percentile(25), '75th': percentile(75), 'min': 'min', 'max': 'max'}})\n",
    "final_distribution_product.columns = final_distribution_product.columns.droplevel(0)\n",
    "final_distribution_product = final_distribution_product[['min', '10th', '25th', 'median', '75th', 'max']]\n",
    "final_distribution_product.to_csv(csv_file_name+' - final_distribution_product.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "# Final distribution by type\n",
    "final_distribution_type = final_data[final_data['outlier_flag']==0].groupby(['groupname', 'typename']).agg({'price_per_each':{'count_type': 'count', 'median': 'median', '10th': percentile(10), '25th': percentile(25), '75th': percentile(75), 'min': 'min', 'max': 'max'}})\n",
    "final_distribution_type.columns = final_distribution_type.columns.droplevel(0)\n",
    "final_distribution_type = final_distribution_type[['min', '10th', '25th', 'median', '75th', 'max']]\n",
    "final_distribution_type.to_csv(csv_file_name+' - final_distribution_type.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Run rest of sections to export sql update statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate script to update e/uom (separate update statements for each eaches_per_uom value)\n",
    "text_file_1 = open(csv_file_name+' - update statements eaches_per_uom - '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.txt', 'w')\n",
    "ids_and_values = e_per_uom_changed[['id','eaches_per_uom']]\n",
    "values_list = e_per_uom_changed['eaches_per_uom'].unique()\n",
    "ids_and_values_dictionary = {}\n",
    "for value in values_list:\n",
    "    ids_and_values_dictionary[value] = ids_and_values['id'].loc[ids_and_values['eaches_per_uom']==value].values\n",
    "for key, value in ids_and_values_dictionary.items():\n",
    "    if len(value) == 0:\n",
    "        pass\n",
    "    elif len(value) == 1:\n",
    "        print('SELECT * FROM po_data \\n --UPDATE po_data SET updated = current_timestamp, updated_by_id =',your_id,',','eaches_per_uom =  ',key,',price_per_each = extended_cost / (units *',key,') \\n WHERE id in\\n(', file = text_file_1)\n",
    "        for x in value:\n",
    "            print(x,'\\n ) \\n ;',file = text_file_1)\n",
    "    else:     \n",
    "        print('SELECT * FROM po_data \\n --UPDATE po_data SET updated = current_timestamp, updated_by_id =',your_id,',','eaches_per_uom =  ',key,',price_per_each = extended_cost / (units *',key,') \\n WHERE id in\\n(', file = text_file_1)\n",
    "        for x in value[:-1]:\n",
    "            print(x,',',file = text_file_1)\n",
    "        print(value[-1],'\\n ) \\n ;',file = text_file_1)\n",
    "text_file_1.close()\n",
    "    \n",
    "#Generate script to update flag to exclude from benchmarking (1 update statement for all)\n",
    "if CLEAN_UP_TYPE == 'opportunity':\n",
    "    id_list_outliers = new_outliers['id']\n",
    "    if len(id_list_outliers) == 0:\n",
    "        pass\n",
    "    elif len(id_list_outliers) ==1: \n",
    "        text_file_2 = open(csv_file_name+' - update statements set exclude_from_benchmarking = TRUE - '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.txt', 'w')\n",
    "        print('SELECT * FROM po_data \\n --UPDATE po_data SET updated = current_timestamp, updated_by_id =',your_id,',','exclude_from_benchmarking = TRUE \\n WHERE id in \\n(', file = text_file_2)\n",
    "        for row in id_list_outliers:\n",
    "            print(row, '\\n ) \\n ;', file = text_file_2)\n",
    "        text_file_2.close()\n",
    "    else: \n",
    "        text_file_2 = open(csv_file_name+' - update statements set exclude_from_benchmarking = TRUE - '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.txt', 'w')\n",
    "        print('SELECT * FROM po_data \\n --UPDATE po_data SET updated = current_timestamp, updated_by_id =',your_id,',','exclude_from_benchmarking = TRUE \\n WHERE id in \\n(', file = text_file_2)\n",
    "        for row in id_list_outliers[:-1]:\n",
    "            print(row,',\\n', file = text_file_2)\n",
    "        print(id_list_outliers.iloc[-1],'\\n ) \\n ;', file = text_file_2)\n",
    "        text_file_2.close()\n",
    "else:\n",
    "    id_list_outliers = new_outliers['id']\n",
    "    if len(id_list_outliers) == 0:\n",
    "        pass\n",
    "    elif len(id_list_outliers) == 1:\n",
    "        text_file_2 = open(csv_file_name+' - update statements set exclude_from_roi = TRUE - '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.txt', 'w')\n",
    "        print('INSERT INTO exclude_roi (data_id) values \\n (', file = text_file_2)\n",
    "        for row in id_list_outliers:\n",
    "            print(row, '\\n ) \\n ;', file = text_file_2)\n",
    "        text_file_2.close()\n",
    "    else:\n",
    "        text_file_2 = open(csv_file_name+' - update statements set exclude_from_roi = TRUE - '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.txt', 'w')\n",
    "        print('INSERT INTO exclude_roi (data_id) values \\n', file = text_file_2)\n",
    "        for row in id_list_outliers[:-1]:\n",
    "            print('(',row,'),', file = text_file_2)\n",
    "        print('(',id_list_outliers.iloc[-1], ')\\n;', end = '', file = text_file_2)\n",
    "        text_file_2.close()\n",
    "\n",
    "#Generate script to update flag to NO LONGER exclude from benchmarking (1 update statement for all)\n",
    "if CLEAN_UP_TYPE == 'opportunity':\n",
    "    id_list_not_outliers = no_longer_outliers['id']\n",
    "    if len(id_list_not_outliers) == 0:\n",
    "        pass\n",
    "    elif len(id_list_not_outliers) == 1:\n",
    "        text_file_3 = open(csv_file_name+' - update statements set exclude_from_benchmarking = FALSE - '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.txt', 'w')\n",
    "        print('SELECT * FROM po_data \\n --UPDATE po_data SET updated = current_timestamp, updated_by_id =',your_id,',','exclude_from_benchmarking = FALSE \\n WHERE id in \\n(', file = text_file_3)\n",
    "        for row in id_list_not_outliers:\n",
    "            print(row, '\\n ) \\n ;' , file = text_file_3)\n",
    "        text_file_3.close()\n",
    "    else:\n",
    "        text_file_3 = open(csv_file_name+' - update statements set exclude_from_benchmarking = FALSE - '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.txt', 'w')\n",
    "        print('SELECT * FROM po_data \\n --UPDATE po_data SET updated = current_timestamp, updated_by_id =',your_id,',','exclude_from_benchmarking = FALSE \\n WHERE id in \\n(', file = text_file_3)\n",
    "        for row in id_list_not_outliers[:-1]:\n",
    "            print(row,',', file = text_file_3)\n",
    "        print(id_list_not_outliers.iloc[-1], '\\n ) \\n ;', file = text_file_3)\n",
    "        text_file_3.close()\n",
    "else:\n",
    "    id_list_not_outliers = no_longer_outliers['id']\n",
    "    if len(id_list_not_outliers) == 0:\n",
    "        pass\n",
    "    elif len(id_list_not_outliers) == 1:\n",
    "        text_file_3 = open(csv_file_name+' - update statements set exclude_from_roi = FALSE - '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.txt', 'w')\n",
    "        print('DELETE FROM exclude_roi \\n WHERE data_id in \\n(', file = text_file_3)\n",
    "        for row in id_list_not_outliers:\n",
    "            print(row, '\\n ) \\n ;', end = '' , file = text_file_3)\n",
    "        text_file_3.close()\n",
    "    else:\n",
    "        text_file_3 = open(csv_file_name+' - update statements set exclude_from_roi = FALSE - '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.txt', 'w')\n",
    "        print('DELETE FROM exclude_roi \\n WHERE data_id in \\n(', file = text_file_3)\n",
    "        for row in id_list_not_outliers[:-1]:\n",
    "            print(row,',', file = text_file_3)\n",
    "        print(id_list_not_outliers.iloc[-1], '\\n ) \\n ;',end = '', file = text_file_3)\n",
    "        text_file_3.close()    \n",
    "\n",
    "\n",
    "# Generate script to insert po_data_clean_up_tracker ids\n",
    "id_list_insert_clean_up = insert_clean_up_tracker['id']\n",
    "if len(id_list_insert_clean_up) == 0:\n",
    "    pass\n",
    "elif len(id_list_insert_clean_up) ==1: \n",
    "    text_file_4 = open(csv_file_name+' - insert ids into po_data_clean_up_tracker - '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.txt', 'w')\n",
    "    print('INSERT INTO po_data_clean_up_tracker (data_id) values \\n(', file = text_file_4)\n",
    "    for row in id_list_insert_clean_up:\n",
    "        print(row, '\\n ) \\n ;', file = text_file_4)\n",
    "    text_file_4.close()\n",
    "else: \n",
    "    text_file_4 = open(csv_file_name+' - insert ids into po_data_clean_up_tracker - '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.txt', 'w')\n",
    "    print('INSERT INTO po_data_clean_up_tracker (data_id) values \\n', file = text_file_4)\n",
    "    for row in id_list_insert_clean_up[:-1]:\n",
    "        print('(',row,'),', file = text_file_4)\n",
    "    print('(',id_list_insert_clean_up.iloc[-1], ') \\n ;', file = text_file_4)\n",
    "    text_file_4.close()\n",
    "\n",
    "# Generate script to update po_data_clean_up_tracker ids\n",
    "if CLEAN_UP_TYPE == 'opportunity':\n",
    "    id_list_update_clean_up = final_data['id']\n",
    "    if len(id_list_update_clean_up) == 0:\n",
    "        pass\n",
    "    elif len(id_list_update_clean_up) ==1: \n",
    "        text_file_5 = open(csv_file_name+' - update po_data_clean_up_tracker - '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.txt', 'w')\n",
    "        print('SELECT * FROM po_data_clean_up_tracker \\n --UPDATE po_data_clean_up_tracker SET opportunity_level = current_timestamp, opportunity_user_id =',your_id,'\\n WHERE data_id in \\n(', file = text_file_5)\n",
    "        for row in id_list_update_clean_up:\n",
    "            print(row, '\\n ) \\n ;', file = text_file_5)\n",
    "        text_file_5.close()\n",
    "    else: \n",
    "        text_file_5 = open(csv_file_name+' - update po_data_clean_up_tracker - '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.txt', 'w')\n",
    "        print('SELECT * FROM po_data_clean_up_tracker \\n --UPDATE po_data_clean_up_tracker SET opportunity_level = current_timestamp, opportunity_user_id =',your_id,'\\n WHERE data_id in \\n(', file = text_file_5)\n",
    "        for row in id_list_update_clean_up[:-1]:\n",
    "            print(row,',', file = text_file_5)\n",
    "        print(id_list_update_clean_up.iloc[-1], '\\n ) \\n ;', file = text_file_5)\n",
    "        text_file_5.close()\n",
    "else: \n",
    "    id_list_update_clean_up = final_data['id']\n",
    "    if len(id_list_update_clean_up) == 0:\n",
    "        pass\n",
    "    elif len(id_list_update_clean_up) ==1: \n",
    "        text_file_5 = open(csv_file_name+' - update po_data_clean_up_tracker - '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.txt', 'w')\n",
    "        print('SELECT * FROM po_data_clean_up_tracker \\n --UPDATE po_data_clean_up_tracker SET dashboard_level = current_timestamp, dashboard_user_id =',your_id,'\\n WHERE data_id in \\n(', file = text_file_5)\n",
    "        for row in id_list_update_clean_up:\n",
    "            print(row, '\\n ) \\n ;', file = text_file_5)\n",
    "        text_file_5.close()\n",
    "    else: \n",
    "        text_file_5 = open(csv_file_name+' - update po_data_clean_up_tracker - '+datetime.datetime.now().strftime(\"%Y-%m-%d\")+'.txt', 'w')\n",
    "        print('SELECT * FROM po_data_clean_up_tracker \\n --UPDATE po_data_clean_up_tracker SET dashboard_level = current_timestamp, dashboard_user_id =',your_id,'\\n WHERE data_id in \\n(', file = text_file_5)\n",
    "        for row in id_list_update_clean_up[:-1]:\n",
    "            print(row,',', file = text_file_5)\n",
    "        print(id_list_update_clean_up.iloc[-1], '\\n ) \\n ;', file = text_file_5)\n",
    "        text_file_5.close()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
