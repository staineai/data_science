{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Outliers Detection ML Model \n",
    "The following code was created for use by Lumere data analysts. They need to quickly clean clients' purchase order data for analysis by identifying and excluding outliers from the analysis dataset. This script utilizes a random forest model to classify purchase orders as an outlier. \n",
    "\n",
    "NOTE: any queries and table names have been blinded to protect Lumere's data. If you wish to run this script, you will have to import your own dataset. \n",
    "\n",
    "### Set up environment \n",
    "The analysts need to pull the relevant data from Lumere's database. This script utilizes the psycopg2 library to pull data from PostgreSQL. The command `getpass` allows the user to enter their database password without saving it to the actual file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import getpass\n",
    "import psycopg2\n",
    "\n",
    "print (\"Enter your database Password\")\n",
    "PASSWORD = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model\n",
    "#### Set Up: \n",
    "First, the user needs to update the parameters, including username, user ID, and the relevant data categories. Next, the script defines a function that takes the output from `fetchall` and inserts it into a pandas dataframe.\n",
    "\n",
    "#### Data: \n",
    "The analysts will use client purchase order data. This data consists of individual purchase orders. Each purchase order has a price, units, unit of measure, eaches per unit of measure, total cost, catalog number, product, and type. The price is calculated by dividing total cost by (units x eaches per unit of measure). Think of a unit of measure as a box of markers and an eaches per unit of measure as the number of markers that comes in a box. Each catalog number can have several valid eaches per unit of measure values. Often, this particular field will be blank or incorrect. This script helps identify these lines and aims to correct the eaches per unit of measure field and recalculate the new price. \n",
    "\n",
    "From this data, the model computes the following features: \n",
    "1. The difference between the purchase order price and the average price for the purchase order's catalog number \n",
    "2. The difference between the purchase order price and the average price for the purchase order's type \n",
    "3. The count of unique purchase orders by catalog number and price \n",
    "4. The count of unique purchase orders by type and price \n",
    "\n",
    "The classifier for this model is `exclude_from_benchmarking`, which is a boolean. `exclude_from_benchmarking`=True when the purchase order is an outlier. \n",
    "\n",
    "#### Model:\n",
    "The model loops over each category ID and does the following: \n",
    "1. The user will run a random forest model over the training and validation data (any data where `verified`=True). \n",
    "2. From this model, the script generates performance metrics. \n",
    "3. If recall > 0.8, then the user runs the model over the unverified data. \n",
    "4. For any purchase orders that were marked as an outlier by the model, the script then recalculates the price based on any additional eaches per unit of measure values for each purchase order and runs the model over that line again to see if a different eaches per unit of measure would make the purchase order not an outlier. \n",
    "5. Finally, the script inserts all predictions and eaches per unit of measure updates into the database for an analyst to verify. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update parameters \n",
    "USERNAME = 'astaines'\n",
    "USER_ID = 12345\n",
    "_CATEGORY_ID_ = [238]\n",
    "\n",
    "# Define a function that takes the output from executing the SQL query and puts it into a pandas dataframe \n",
    "def fetch(cur):\n",
    "    df = pd.DataFrame(np.array(cur.fetchall()))\n",
    "    colnames = [desc[0] for desc in cur.description]\n",
    "    df.columns = colnames\n",
    "    return df\n",
    "\n",
    "# Connect to the database\n",
    "con = psycopg2.connect(dbname = 'dbname',\\\n",
    "                host = 'host.com',\\\n",
    "                user = USERNAME,\\\n",
    "                password = PASSWORD_WAR,\\\n",
    "                port = 'port')\n",
    "cur = con.cursor()\n",
    "print('Connected to database')\n",
    "\n",
    "# Loop over all categories in __CATEGORY_ID__ list \n",
    "for cat in range(len(_CATEGORY_ID_)):\n",
    "    # Set category ID\n",
    "    CATEGORY_ID = _CATEGORY_ID_[cat]\n",
    "\n",
    "    # Get the most recent model_id + 1 from the database to use as model_id \n",
    "    cur.execute('''SELECT max(model_id)+1 as model_id FROM data table;''')\n",
    "    model_id = fetch(cur)\n",
    "    model_id = model_id.loc[0,'model_id']\n",
    "\n",
    "    # Pull the category groups ID's \n",
    "    cur.execute('''SELECT id FROM data table \n",
    "    WHERE product_category_id IN ({0}) AND analytics_available = 2;'''.format(CATEGORY_ID))\n",
    "    groups = fetch(cur)\n",
    "\n",
    "    # Pull data from pg-warehouse \n",
    "    # The model uses the following features - price per each, the difference between price per each and the average price \n",
    "    # by catalog number and type, and the count of each unique price per each by catalog and type. \n",
    "    # The first section of this query pulls only data that has been verified, which will be used for the training and \n",
    "    # validation datasets. The second section pulls the rest of the data, which we will generate predictions for. \n",
    "    # This query also pulls in exclude_from_benchmarking, which the variable we will be predicting, and the line_id.\n",
    "    print('Pulling data for category ' + str(CATEGORY_ID) + '...')\n",
    "    cur.execute('''SELECT\n",
    "      'training' AS data_set,\n",
    "      type_id, \n",
    "      product_id, \n",
    "      catalog_number, \n",
    "      price_per_each,\n",
    "      eaches_per_uom,\n",
    "      units, \n",
    "      total_cost,\n",
    "      NULL AS allowable_e_per_uom, \n",
    "      line_id,\n",
    "      exclude_from_benchmarking,\n",
    "      catalog_avg_price, \n",
    "      type_avg_price,\n",
    "      coalesce((price_per_each - catalog_avg_price) / catalog_avg_price, -1) AS catalog_price_diff,\n",
    "      coalesce((price_per_each - type_avg_price) / type_avg_price, -1) AS type_price_diff,\n",
    "      coalesce(catalog_price_count, 0) AS catalog_price_count,\n",
    "      coalesce(type_price_count, 0)AS type_price_count\n",
    "    FROM data table\n",
    "    WHERE category_id IN ({0})\n",
    "        AND verified=True\n",
    "    UNION\n",
    "    SELECT\n",
    "      'testing' AS data_set,\n",
    "      type_id, \n",
    "      product_id, \n",
    "      catalog_number, \n",
    "      price_per_each,\n",
    "      eaches_per_uom,\n",
    "      units, \n",
    "      total_cost,\n",
    "      NULL AS allowable_e_per_uom, \n",
    "      line_id,\n",
    "      exclude_from_benchmarking,\n",
    "      catalog_avg_price, \n",
    "      type_avg_price,\n",
    "      coalesce((price_per_each - catalog_avg_price) / catalog_avg_price, -1) AS catalog_price_diff,\n",
    "      coalesce((price_per_each - type_avg_price) / type_avg_price, -1) AS type_price_diff,\n",
    "      coalesce(catalog_price_count, 0) AS catalog_price_count,\n",
    "      coalesce(type_price_count, 0)AS type_price_count\n",
    "    FROM data table\n",
    "    WHERE category_id IN ({0})\n",
    "        AND verified=False;'''.format(CATEGORY_ID))\n",
    "    data = fetch(cur)\n",
    "    print('Training and testing data pulled from database')\n",
    "\n",
    "    # Set the index to the line_id to be able to trace predictions back to actual data \n",
    "    data = data.set_index('line_id')\n",
    "\n",
    "    # Define training and validation data\n",
    "    training_data = data.loc[data['data_set']=='training',]\n",
    "    testing_data = data.loc[data['data_set']=='testing',]\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    class_le = LabelEncoder()\n",
    "\n",
    "    y = class_le.fit_transform(training_data['exclude_from_benchmarking'].values)\n",
    "    X = training_data[['price_per_each','catalog_price_diff', 'type_price_diff', 'catalog_price_count', 'type_price_count']]\n",
    "\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
    "\n",
    "    y_predict = class_le.fit_transform(testing_data['exclude_from_benchmarking'].values)\n",
    "    X_predict = testing_data[['price_per_each','catalog_price_diff', 'type_price_diff', 'catalog_price_count', 'type_price_count']]\n",
    "\n",
    "    # Fit the model using a Random Forest Classifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    forest = RandomForestClassifier(criterion = 'entropy', n_estimators=1000, class_weight = 'balanced')\n",
    "    forest.fit(X_train, y_train)\n",
    "    print('Model fitted to training data')\n",
    "\n",
    "    # Generate performance metrics\n",
    "    y_pred = forest.predict(X_test)\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    con_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "    if len(con_mat) == 1: \n",
    "        precision = 1\n",
    "        recall = 1\n",
    "    else: \n",
    "        precision = con_mat[1,1]/(con_mat[0,1]+con_mat[1,1])\n",
    "        recall = con_mat[1,1]/(con_mat[1,0]+con_mat[1,1])\n",
    "\n",
    "    print('Recall: ' + str(recall))\n",
    "\n",
    "    # Write output to database tables\n",
    "    # Generate the outliers_groups insert statement\n",
    "    insert_groups = ''\n",
    "    if len(groups) == 1: \n",
    "        insert_groups += '({0}, {1}, {2})'.format(model_id, CATEGORY_ID, groups.loc[0,'id'])\n",
    "    else: \n",
    "        for group in range(len(groups)-1):\n",
    "            new_group = '({0}, {1}, {2}),'.format(model_id, CATEGORY_ID, groups.loc[group,'id'])\n",
    "            insert_groups += new_group\n",
    "\n",
    "        insert_groups += '({0}, {1}, {2})'.format(model_id, CATEGORY_ID, groups.loc[len(groups)-1,'id'])\n",
    "\n",
    "    # Insert the model's performance metrics into outliers_model \n",
    "    cur.execute('''INSERT INTO data table (model_id, run_date, ran_by_id, precision, recall) VALUES\n",
    "      ({0},current_timestamp, {1}, {2}, {3})'''.format(model_id, USER_ID, precision, recall))\n",
    "\n",
    "    # Insert the model category and groups into outliers_groups \n",
    "    cur.execute('''INSERT INTO data table (model_id, category_id, group_id) VALUES '''+ str(insert_groups))\n",
    "\n",
    "    con.commit()\n",
    "    print('Model performance metrics and groups successfully inserted into database')\n",
    "\n",
    "    if len(con_mat) == 1:\n",
    "        # If the testing dataset did not have any outliers then the len(con_mat) will = 1\n",
    "        print('No outliers in training or validation data - model cannot accurately predict outliers')\n",
    "    elif recall >= 0.8: \n",
    "        # We will only insert predictions into outliers_predictions for models that have a recall > 0.8 \n",
    "        # Generate predictions\n",
    "        y_pred = forest.predict(X_predict)\n",
    "        probabilities = forest.predict_proba(X_predict[0:])\n",
    "\n",
    "        predictions = X_predict\n",
    "        predictions['prediction'] = y_pred\n",
    "        predictions['probability'] = probabilities[:,1]\n",
    "\n",
    "        # Merge predictions with testing_data in order to get the line_id for each prediction\n",
    "        predictions = pd.merge(testing_data[['group_type_id', 'product_id', 'catalog_stripped', 'exclude_from_benchmarking', 'eaches_per_uom', 'allowable_e_per_uom', 'units', 'extended_cost', 'catalog_avg_price', 'type_avg_price']], predictions, left_index=True, right_index=True)\n",
    "        predictions = predictions.reset_index(drop=False)\n",
    "\n",
    "        predictions.loc[predictions['prediction']==0, 'prediction'] = False\n",
    "        predictions.loc[predictions['prediction']==1, 'prediction'] = True\n",
    "\n",
    "        # Iterate over the predicted outliers to see if we can fix any eaches per uom problems \n",
    "        outliers = predictions.loc[predictions['prediction']==True].reset_index(drop=True)\n",
    "        outliers.loc[outliers['allowable_e_per_uom'].isnull(),'allowable_e_per_uom'] = 1\n",
    "        \n",
    "        for i in range(len(outliers)):\n",
    "            if outliers['allowable_e_per_uom'][i]==1: \n",
    "                outliers['allowable_e_per_uom'][i]=[1]\n",
    "            for j in range(len(outliers['allowable_e_per_uom'][i])): \n",
    "                col_name = 'allowable_e_per_uom_' + str(j)\n",
    "                outliers.loc[outliers.index==i,col_name] = outliers['allowable_e_per_uom'][i][j]\n",
    "\n",
    "        allowable_e_per_uom_cols = outliers.filter(like='allowable_e_per_uom_').columns\n",
    "\n",
    "        for i in range(len(allowable_e_per_uom_cols)):\n",
    "            allowable_e_per_uom_col = 'allowable_e_per_uom_' + str(i)\n",
    "            price_per_each_col = 'price_per_each_' + str(i)\n",
    "            catalog_price_diff_col = 'catalog_price_diff_' + str(i)\n",
    "            type_price_diff_col = 'type_price_diff_' + str(i)\n",
    "            catalog_price_count_col = 'catalog_price_count_' + str(i)\n",
    "            type_price_count_col = 'type_price_count_' + str(i)\n",
    "            prediction_col = 'prediction_' + str(i)\n",
    "            probability_col = 'probability_' + str(i)\n",
    "\n",
    "            outliers[price_per_each_col] = outliers['extended_cost'].astype(float)/(outliers[allowable_e_per_uom_col].astype(float)*outliers['units'].astype(float))\n",
    "            outliers.loc[outliers[price_per_each_col].isnull(), price_per_each_col] = 0 \n",
    "\n",
    "            outliers[catalog_price_diff_col] = (outliers[price_per_each_col].astype(float) - outliers['catalog_avg_price'].astype(float))/outliers['catalog_avg_price'].astype(float)\n",
    "            outliers[type_price_diff_col] = (outliers[price_per_each_col].astype(float) - outliers['type_avg_price'].astype(float))/outliers['type_avg_price'].astype(float)\n",
    "            outliers.loc[outliers[catalog_price_diff_col].isnull(), catalog_price_diff_col] = -1\n",
    "            outliers.loc[outliers[type_price_diff_col].isnull(), type_price_diff_col] = -1\n",
    "\n",
    "            outliers = pd.merge(outliers, training_data[['product_id', 'catalog_stripped', 'price_per_each', 'catalog_price_count']].drop_duplicates(), how='left', left_on=['product_id', 'catalog_stripped', price_per_each_col], right_on=['product_id', 'catalog_stripped', 'price_per_each'])\n",
    "            outliers = pd.merge(outliers, training_data[['group_type_id', 'price_per_each', 'type_price_count']].drop_duplicates(), how='left', left_on=['group_type_id', price_per_each_col], right_on=['group_type_id', 'price_per_each'])\n",
    "            outliers = outliers.rename(columns = {'catalog_price_count_x': 'catalog_price_count'})\n",
    "            outliers = outliers.rename(columns = {'type_price_count_x': 'type_price_count'})\n",
    "            outliers = outliers.rename(columns = {'catalog_price_count_y': catalog_price_count_col})\n",
    "            outliers = outliers.rename(columns = {'type_price_count_y': type_price_count_col})\n",
    "            outliers = outliers.drop(['price_per_each', 'price_per_each_y'], axis=1)\n",
    "            outliers = outliers.rename(columns = {'price_per_each_x': 'price_per_each'})\n",
    "\n",
    "            outliers.loc[outliers[catalog_price_count_col].isnull(), catalog_price_count_col] = 0\n",
    "            outliers.loc[outliers[type_price_count_col].isnull(), type_price_count_col] = 0\n",
    "\n",
    "            X_predict_e = outliers[[price_per_each_col, catalog_price_diff_col, type_price_diff_col, catalog_price_count_col, type_price_count_col]]\n",
    "            y_pred_e = forest.predict(X_predict_e)\n",
    "            probabilities_e = forest.predict_proba(X_predict_e[0:])\n",
    "            outliers[prediction_col] = y_pred_e\n",
    "            outliers.loc[outliers[prediction_col]==0, prediction_col] = False\n",
    "            outliers.loc[outliers[prediction_col]==1, prediction_col] = True\n",
    "            outliers[probability_col] = probabilities_e[:,1]\n",
    "\n",
    "            predictions = pd.merge(predictions, outliers.loc[outliers[prediction_col]==False,('line_id',allowable_e_per_uom_col, probability_col)], how='left', on='line_id')\n",
    "            predictions.loc[~predictions[allowable_e_per_uom_col].isnull(), 'prediction'] = False\n",
    "            predictions.loc[~predictions[allowable_e_per_uom_col].isnull(), 'update_e_per_uom'] = predictions[allowable_e_per_uom_col]\n",
    "            predictions.loc[~predictions[probability_col].isnull(), 'probability'] = predictions[probability_col]\n",
    "            predictions = predictions.drop([probability_col, allowable_e_per_uom_col], axis=1)\n",
    "\n",
    "        predictions.loc[predictions['update_e_per_uom'].isnull(), 'update_e_per_uom'] = 'NULL'\n",
    "        \n",
    "        # Generate the insert statement for the predictions as a string \n",
    "        insert_predictions = ''\n",
    "        for line_id in range(len(predictions)-1):\n",
    "            new_prediction = '({0}, {1}, {2}, {3}, {4}, {5}),'.format(model_id, predictions.loc[line_id,'line_id'], predictions.loc[line_id,'exclude_from_benchmarking'],predictions.loc[line_id,'prediction'], predictions.loc[line_id,'probability'], predictions.loc[line_id,'update_e_per_uom'])\n",
    "            insert_predictions += new_prediction\n",
    "\n",
    "        insert_predictions += '({0}, {1}, {2}, {3}, {4}, {5})'.format(model_id, predictions.loc[len(predictions)-1,'line_id'], predictions.loc[len(predictions)-1,'exclude_from_benchmarking'],predictions.loc[len(predictions)-1,'prediction'], predictions.loc[len(predictions)-1,'probability'], predictions.loc[line_id,'update_e_per_uom'])\n",
    "\n",
    "        # Insert predictions into outliers_predictions \n",
    "        cur.execute('''INSERT INTO data table (model_id, line_id, original_value, prediction, probability, update_e_per_uom) VALUES '''+ str(insert_predictions))\n",
    "\n",
    "        con.commit()\n",
    "\n",
    "        print('Model predictions successfully inserted into database')\n",
    "    else: \n",
    "        print('Model did not meet requirements - recall < 0.8')\n",
    "\n",
    "    print('Category ' + str(CATEGORY_ID) + ' run complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36x64",
   "language": "python",
   "name": "py36x64"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
